{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape news tables by ticker from finviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from loky import get_reusable_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "sandp_df = table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read news table from finviz (use for process pool executor)\n",
    "def finviz_news_table_process(ticker):\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    \n",
    "    try:\n",
    "        url = finviz_url + ticker\n",
    "        req = Request(url=url, headers={'user-agent': 'my-app/0.0.1'})\n",
    "        response = urlopen(req)\n",
    "        html = BeautifulSoup(response)\n",
    "        news_table = str(html.find(id='news-table'))\n",
    "    except:\n",
    "        news_table = None\n",
    "        \n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    # Return [ticker, str_news_table, run_time, pid]\n",
    "    return [ticker, news_table, end_time - start_time, pid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker    pid  run_time                                     str_news_table\n",
      "0    MMM  14964  2.288448  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "1    ABT  16988  3.084255  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "2   ABBV   8264  2.243675  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "3   ABMD   7572  1.954463  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "4    ACN   4652  2.769137  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "5   ATVI   6940  3.098437  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "6   ADBE   4472  3.458757  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "7    AMD  15928  3.273517  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "8    AAP   8324  2.407815  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "9    AES   8052  2.696584  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "Process Pool Executor finished in:  100.13702599999999  seconds\n"
     ]
    }
   ],
   "source": [
    "# Process Pool Executor: read html from finviz for each ticker and save the news_table of each as a dataframe\n",
    "\n",
    "start_test1 = time.perf_counter()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "    ticker_list = sandp_df['Symbol']\n",
    "    # initiate executor\n",
    "    executor = get_reusable_executor(max_workers=10, timeout=5)\n",
    "    # apply executor to map finviz_news_table on ticker list \n",
    "    process_1 = executor.map(finviz_news_table_process, ticker_list)\n",
    "    # save news tables as a dataframe (includes run time for each request)\n",
    "    news_table_df = pd.DataFrame([[ticker, pid, run_time, str_news_table] for ticker, str_news_table, run_time, pid in process_1], columns=['ticker', 'pid', 'run_time', 'str_news_table'])\n",
    "    print(news_table_df.head(10))\n",
    "    \n",
    "\n",
    "end_test1 = time.perf_counter()\n",
    "\n",
    "print('Process Pool Executor finished in: ', end_test1 - start_test1, ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List each article with its attached date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_list(html_article_list):\n",
    "    date_time = []\n",
    "    for html_article in html_article_list:\n",
    "        date_scrape = html_article.td.text.split()\n",
    "        if len(date_scrape) == 1:\n",
    "            time_ = date_scrape[0]\n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time_ = date_scrape[1]\n",
    "        date_time.append([date, time_])\n",
    "        \n",
    "        \n",
    "    return date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_articles(row):\n",
    "    \n",
    "    html_news_table = BeautifulSoup(row[3], 'html.parser')\n",
    "    \n",
    "    article_list = html_news_table.findAll('tr')\n",
    "    \n",
    "    date_time_list = date_to_list(article_list)\n",
    "    \n",
    "    article_list = pd.Series([str(x) for x in article_list])\n",
    "    \n",
    "    ticker_list = pd.Series([row[0]] * len(article_list))\n",
    "    \n",
    "    articles_df = pd.DataFrame(date_time_list, columns=['date', 'time'])\n",
    "    articles_df.insert(0, 'ticker', ticker_list)\n",
    "    articles_df['raw_article'] = article_list\n",
    "    \n",
    "    \n",
    "    return articles_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>raw_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>Feb-19-21</td>\n",
       "      <td>08:11AM</td>\n",
       "      <td>&lt;tr&gt;&lt;td align=\"right\" style=\"white-space:nowra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MMM</td>\n",
       "      <td>Feb-18-21</td>\n",
       "      <td>11:00AM</td>\n",
       "      <td>&lt;tr&gt;&lt;td align=\"right\" style=\"white-space:nowra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MMM</td>\n",
       "      <td>Feb-17-21</td>\n",
       "      <td>09:37AM</td>\n",
       "      <td>&lt;tr&gt;&lt;td align=\"right\" style=\"white-space:nowra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MMM</td>\n",
       "      <td>Feb-16-21</td>\n",
       "      <td>04:18PM</td>\n",
       "      <td>&lt;tr&gt;&lt;td align=\"right\" style=\"white-space:nowra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MMM</td>\n",
       "      <td>Feb-16-21</td>\n",
       "      <td>03:58PM</td>\n",
       "      <td>&lt;tr&gt;&lt;td align=\"right\" width=\"130\"&gt;03:58PM  &lt;/t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Jun-08-20</td>\n",
       "      <td>10:41AM</td>\n",
       "      <td>&lt;tr&gt;&lt;td align=\"right\" style=\"white-space:nowra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Jun-08-20</td>\n",
       "      <td>08:30AM</td>\n",
       "      <td>&lt;tr&gt;&lt;td align=\"right\" width=\"130\"&gt;08:30AM  &lt;/t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Jun-05-20</td>\n",
       "      <td>11:32AM</td>\n",
       "      <td>&lt;tr&gt;&lt;td align=\"right\" style=\"white-space:nowra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Jun-05-20</td>\n",
       "      <td>08:14AM</td>\n",
       "      <td>&lt;tr&gt;&lt;td align=\"right\" width=\"130\"&gt;08:14AM  &lt;/t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Jun-03-20</td>\n",
       "      <td>10:00AM</td>\n",
       "      <td>&lt;tr&gt;&lt;td align=\"right\" style=\"white-space:nowra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50071 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticker       date     time  \\\n",
       "0     MMM  Feb-19-21  08:11AM   \n",
       "1     MMM  Feb-18-21  11:00AM   \n",
       "2     MMM  Feb-17-21  09:37AM   \n",
       "3     MMM  Feb-16-21  04:18PM   \n",
       "4     MMM  Feb-16-21  03:58PM   \n",
       "..    ...        ...      ...   \n",
       "95    ZTS  Jun-08-20  10:41AM   \n",
       "96    ZTS  Jun-08-20  08:30AM   \n",
       "97    ZTS  Jun-05-20  11:32AM   \n",
       "98    ZTS  Jun-05-20  08:14AM   \n",
       "99    ZTS  Jun-03-20  10:00AM   \n",
       "\n",
       "                                          raw_article  \n",
       "0   <tr><td align=\"right\" style=\"white-space:nowra...  \n",
       "1   <tr><td align=\"right\" style=\"white-space:nowra...  \n",
       "2   <tr><td align=\"right\" style=\"white-space:nowra...  \n",
       "3   <tr><td align=\"right\" style=\"white-space:nowra...  \n",
       "4   <tr><td align=\"right\" width=\"130\">03:58PM  </t...  \n",
       "..                                                ...  \n",
       "95  <tr><td align=\"right\" style=\"white-space:nowra...  \n",
       "96  <tr><td align=\"right\" width=\"130\">08:30AM  </t...  \n",
       "97  <tr><td align=\"right\" style=\"white-space:nowra...  \n",
       "98  <tr><td align=\"right\" width=\"130\">08:14AM  </t...  \n",
       "99  <tr><td align=\"right\" style=\"white-space:nowra...  \n",
       "\n",
       "[50071 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse through every news table and generate a dataframe of each individual article\n",
    "\n",
    "article_df = pd.DataFrame([])\n",
    "for x in range(len(news_table_df)):\n",
    "    if news_table_df.iloc[x][3] == None:\n",
    "        continue\n",
    "    else:\n",
    "        new_ticker = unpack_articles(news_table_df.iloc[x])\n",
    "        article_df = article_df.append(new_ticker)\n",
    "    \n",
    "article_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile details for each article and request content from related sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def details(row):\n",
    "    \n",
    "    ticker, date, time_, str_article = row[0], row[1], row[2], row[3]\n",
    "    \n",
    "    html_article = BeautifulSoup(str_article, 'html.parser')\n",
    "    \n",
    "    # Produce news source company\n",
    "    news = html_article.span.get_text()\n",
    "        \n",
    "    # Produce headlines\n",
    "    headline = html_article.a.get_text()     \n",
    "    \n",
    "    # Produce news content\n",
    "    # get link to the full article\n",
    "    link = html_article.find('a').get('href')\n",
    "    content = 'empty string'\n",
    "    url_root = urlparse(link).netloc\n",
    "    # check if link leads to yahoo.finance\n",
    "    if url_root == 'finance.yahoo.com':\n",
    "        try:\n",
    "            # request from yahoo.finance\n",
    "            req_art = Request(url=link, headers={'user-agent':'my-app/0.0.1'})            \n",
    "            response_art = urlopen(req_art)\n",
    "            html_art = BeautifulSoup(response_art)\n",
    "            # get the article content\n",
    "            content = str(html_art.find(class_='caas-body').get_text())\n",
    "        except:\n",
    "            print('Error following article link: ', link)\n",
    "    \n",
    "    return [ticker, date, time_, headline, news, content, url_root]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 Complete in 209.48304740000003 seconds\n",
      "Batch 1 Complete in 216.72395820000008 seconds\n",
      "Batch 2 Complete in 175.6453194999999 seconds\n",
      "Batch 3 Complete in 191.37775190000002 seconds\n",
      "Batch 4 Complete in 162.39360980000015 seconds\n",
      "Batch 5 Complete in 165.9622233 seconds\n",
      "Batch 6 Complete in 176.97866610000005 seconds\n",
      "Batch 7 Complete in 174.39224519999993 seconds\n",
      "Batch 8 Complete in 195.4282029000001 seconds\n",
      "Batch 9 Complete in 142.94714840000006 seconds\n",
      "Batch 10 Complete in 183.8978119999997 seconds\n",
      "Batch 11 Complete in 196.0864762000001 seconds\n",
      "Batch 12 Complete in 207.92768509999996 seconds\n",
      "Batch 13 Complete in 154.0975487999999 seconds\n",
      "Batch 14 Complete in 179.4179240999997 seconds\n",
      "Batch 15 Complete in 186.14858189999995 seconds\n",
      "Batch 16 Complete in 166.94575040000018 seconds\n",
      "Batch 17 Complete in 186.51190799999995 seconds\n",
      "Batch 18 Complete in 216.33673729999964 seconds\n",
      "Batch 19 Complete in 150.06558930000028 seconds\n",
      "Batch 20 Complete in 208.4596517 seconds\n",
      "Batch 21 Complete in 165.9514212999993 seconds\n",
      "Batch 22 Complete in 175.7526544000002 seconds\n",
      "Batch 23 Complete in 184.38859440000033 seconds\n",
      "Batch 24 Complete in 175.4047013999998 seconds\n",
      "Batch 25 Complete in 170.04293830000006 seconds\n",
      "Batch 26 Complete in 196.9698843999995 seconds\n",
      "Batch 27 Complete in 161.03061639999942 seconds\n",
      "Batch 28 Complete in 185.51163189999988 seconds\n",
      "Batch 29 Complete in 190.29791810000006 seconds\n",
      "Batch 30 Complete in 268.88887929999964 seconds\n",
      "Batch 31 Complete in 211.9318430000003 seconds\n",
      "Batch 32 Complete in 166.7247054999998 seconds\n",
      "Batch 33 Complete in 200.68006199999945 seconds\n",
      "Batch 34 Complete in 162.67457319999994 seconds\n",
      "Batch 35 Complete in 217.53548580000006 seconds\n",
      "Batch 36 Complete in 189.8052292000002 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\anaconda3\\lib\\site-packages\\loky\\process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 37 Complete in 240.5990542999998 seconds\n",
      "Batch 38 Complete in 218.5275142999999 seconds\n",
      "Batch 39 Complete in 191.63415859999895 seconds\n",
      "Batch 40 Complete in 195.23942180000086 seconds\n",
      "Batch 41 Complete in 173.05244590000075 seconds\n",
      "Batch 42 Complete in 201.80349509999905 seconds\n",
      "Batch 43 Complete in 219.28495000000112 seconds\n",
      "Batch 44 Complete in 242.01632860000063 seconds\n",
      "Batch 45 Complete in 192.7729115000002 seconds\n",
      "Batch 46 Complete in 195.91346010000052 seconds\n",
      "Batch 47 Complete in 210.84015399999953 seconds\n",
      "Batch 48 Complete in 180.7509159000001 seconds\n",
      "Batch 49 Complete in 183.90850930000124 seconds\n",
      "Batch 50 Complete in 183.92545020000034 seconds\n",
      "Batch 51 Complete in 217.09037669999998 seconds\n",
      "Batch 52 Complete in 164.74287820000063 seconds\n",
      "Batch 53 Complete in 205.5943987999999 seconds\n",
      "Batch 54 Complete in 173.34849589999976 seconds\n",
      "Batch 55 Complete in 225.87813640000059 seconds\n",
      "Batch 56 Complete in 214.59522430000106 seconds\n",
      "Batch 57 Complete in 177.10558669999955 seconds\n",
      "Batch 58 Complete in 171.9425255999995 seconds\n",
      "Batch 59 Complete in 205.11691149999933 seconds\n",
      "Batch 60 Complete in 183.6794436 seconds\n",
      "Batch 61 Complete in 184.81564809999873 seconds\n",
      "Batch 62 Complete in 186.62796400000116 seconds\n",
      "Batch 63 Complete in 211.78059980000035 seconds\n",
      "Batch 64 Complete in 248.03752990000066 seconds\n",
      "Batch 65 Complete in 230.5408036000008 seconds\n",
      "Batch 66 Complete in 215.1408233000002 seconds\n",
      "Batch 67 Complete in 194.05870270000014 seconds\n",
      "Batch 68 Complete in 154.7622377000007 seconds\n",
      "Batch 69 Complete in 170.2725544999994 seconds\n",
      "Batch 70 Complete in 194.34933610000007 seconds\n",
      "Batch 71 Complete in 175.44287090000034 seconds\n",
      "Batch 72 Complete in 182.94595560000016 seconds\n",
      "Batch 73 Complete in 178.91162480000094 seconds\n",
      "Batch 74 Complete in 191.89236299999902 seconds\n",
      "Batch 75 Complete in 200.07777989999886 seconds\n",
      "Batch 76 Complete in 187.60509600000114 seconds\n",
      "Batch 77 Complete in 173.99101279999923 seconds\n",
      "Batch 78 Complete in 178.92547040000136 seconds\n",
      "Batch 79 Complete in 198.11895079999886 seconds\n",
      "Batch 80 Complete in 165.10694459999831 seconds\n",
      "Batch 81 Complete in 173.71248769999875 seconds\n",
      "Batch 82 Complete in 166.57980219999808 seconds\n",
      "Batch 83 Complete in 162.76756410000235 seconds\n",
      "Batch 84 Complete in 191.980674800001 seconds\n",
      "Batch 85 Complete in 168.91054580000127 seconds\n",
      "Batch 86 Complete in 196.0086384999995 seconds\n",
      "Batch 87 Complete in 176.29812570000286 seconds\n",
      "Batch 88 Complete in 192.52714040000137 seconds\n",
      "Batch 89 Complete in 199.19561819999944 seconds\n",
      "Batch 90 Complete in 163.73166169999968 seconds\n",
      "Batch 91 Complete in 176.66392020000058 seconds\n",
      "Batch 92 Complete in 197.6883474999995 seconds\n",
      "Batch 93 Complete in 161.89608289999887 seconds\n",
      "Batch 94 Complete in 184.68660520000049 seconds\n",
      "Batch 95 Complete in 161.95170540000254 seconds\n",
      "Batch 96 Complete in 179.9167109000009 seconds\n",
      "Batch 97 Complete in 204.62717449999764 seconds\n",
      "Batch 98 Complete in 194.7360654999975 seconds\n",
      "Batch 99 Complete in 189.50004089999857 seconds\n",
      "Scraping Complete\n"
     ]
    }
   ],
   "source": [
    "# scraping articles and saving them as csv in batches of 500\n",
    "\n",
    "for i in range(0,100):\n",
    "    article_list = article_df.iloc[(500*i) : (500*(1+i))]\n",
    "    article_list = article_list.values.tolist()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        executor = get_reusable_executor(max_workers=12, timeout=5)\n",
    "        mapper = executor.map(details, article_list)\n",
    "        raw_list = [x for x in mapper]\n",
    "\n",
    "    raw_df = pd.DataFrame(np.array(raw_list), columns=['ticker', 'date', 'time', 'headline', 'news', 'content', 'site'])\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    if i == 0:\n",
    "        raw_df.to_csv('Raw Data.csv', index=False, header=True)\n",
    "    else:\n",
    "        previous_data = pd.read_csv('Raw Data.csv')\n",
    "        raw_df = previous_data.append(raw_df)\n",
    "        raw_df.to_csv('Raw Data.csv', index=False, header=True)\n",
    "    \n",
    "    print('Batch ' + str(i) + ' Complete in ' + str(end - start) + ' seconds' )\n",
    "\n",
    "\n",
    "article_list = article_df.iloc[50000:].values.tolist()\n",
    "if __name__ == '__main__':\n",
    "    executor = get_reusable_executor(max_workers=12, timeout=5)\n",
    "    mapper = executor.map(details, article_list)\n",
    "    raw_list = [x for x in mapper]\n",
    "\n",
    "raw_df = pd.DataFrame(np.array(raw_list), columns=['ticker', 'date', 'time', 'headline', 'news', 'content', 'site'])\n",
    "previous_data = pd.read_csv('Raw Data.csv')\n",
    "raw_df = previous_data.append(raw_df)\n",
    "raw_df.to_csv('Raw Data.csv', index=False, header=True)\n",
    "print('Scraping Complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split raw data into 3 csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_df = pd.read_csv('Raw Data.csv')\n",
    "n_row = len(raw_df) // 3\n",
    "\n",
    "raw_df1 = raw_df.iloc[:n_row]\n",
    "raw_df2 = raw_df.iloc[n_row:(n_row*2)]\n",
    "raw_df3 = raw_df.iloc[(n_row*2):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df1.to_csv('Data1.csv', index=False, header=True)\n",
    "raw_df2.to_csv('Data2.csv', index=False, header=True)\n",
    "raw_df3.to_csv('Data3.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
