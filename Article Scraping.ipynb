{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape news tables by ticker from finviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from loky import get_reusable_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "sandp_df = table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read news table from finviz (use for process pool executor)\n",
    "def finviz_news_table_process(ticker):\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    \n",
    "    try:\n",
    "        url = finviz_url + ticker\n",
    "        req = Request(url=url, headers={'user-agent': 'my-app/0.0.1'})\n",
    "        response = urlopen(req)\n",
    "        html = BeautifulSoup(response)\n",
    "        news_table = str(html.find(id='news-table'))\n",
    "    except:\n",
    "        news_table = None\n",
    "        \n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    # Return [ticker, str_news_table, run_time, pid]\n",
    "    return [ticker, news_table, end_time - start_time, pid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker    pid  run_time                                     str_news_table\n",
      "0    MMM  22208  1.274536  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "1    ABT  18692  2.922247  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "2   ABBV  19920  3.225352  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "3   ABMD  19000  2.609542  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "4    ACN  21180  5.201069  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "5   ATVI  15716  5.127448  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "6   ADBE  18772  7.464577  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "7    AMD   4592  8.049512  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "8    AAP  22720  7.066407  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "9    AES  21468  6.079866  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "Process Pool Executor finished in:  200.4873933  seconds\n"
     ]
    }
   ],
   "source": [
    "# Process Pool Executor: read html from finviz for each ticker and save the news_table of each as a dataframe\n",
    "\n",
    "start_test1 = time.perf_counter()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "    ticker_list = sandp_df['Symbol']\n",
    "    # initiate executor\n",
    "    executor = get_reusable_executor(max_workers=10, timeout=5)\n",
    "    # apply executor to map finviz_news_table on ticker list \n",
    "    process_1 = executor.map(finviz_news_table_process, ticker_list)\n",
    "    # save news tables as a dataframe (includes run time for each request)\n",
    "    news_table_df = pd.DataFrame([[ticker, pid, run_time, str_news_table] for ticker, str_news_table, run_time, pid in process_1], columns=['ticker', 'pid', 'run_time', 'str_news_table'])\n",
    "    print(news_table_df.head(10))\n",
    "    \n",
    "\n",
    "end_test1 = time.perf_counter()\n",
    "\n",
    "print('Process Pool Executor finished in: ', end_test1 - start_test1, ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape article contents from links on finviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to generate [date, time, headline, news_source, content, article_site] \n",
    "# input is a single article\n",
    "def article_details(date_time, str_article):\n",
    "    \n",
    "    # convert str to html\n",
    "    html_article = BeautifulSoup(str_article, 'html.parser')\n",
    "    \n",
    "    # Produce date and time\n",
    "    date = date_time[0]\n",
    "    time_ = date_time[1]\n",
    "    \n",
    "    # Produce headlines\n",
    "    headline = html_article.a.get_text() \n",
    "    \n",
    "    # Produce news source company\n",
    "    news = html_article.span.get_text()\n",
    "        \n",
    "    # Produce news content\n",
    "    # get link to the full article\n",
    "    link = html_article.find('a').get('href')\n",
    "    content = 'empty string'\n",
    "    url_root = urlparse(link).netloc\n",
    "    # check if link leads to yahoo.finance\n",
    "    if url_root == 'finance.yahoo.com':\n",
    "        try:\n",
    "            # request from yahoo.finance\n",
    "            req_art = Request(url=link, headers={'user-agent':'my-app/0.0.1'})            \n",
    "            response_art = urlopen(req_art)\n",
    "            html_art = BeautifulSoup(response_art)\n",
    "            # get the article content\n",
    "            content = str(html_art.find(class_='caas-body').get_text())\n",
    "        except:\n",
    "            print('Error following article link: ', link)\n",
    "    \n",
    "    # Return [date, time, headline, news_source, content, article_site] \n",
    "    return [date, time_, headline, news, content, url_root]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_list(html_article_list):\n",
    "    date_time = []\n",
    "    for html_article in html_article_list:\n",
    "        date_scrape = html_article.td.text.split()\n",
    "        if len(date_scrape) == 1:\n",
    "            time_ = date_scrape[0]\n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time_ = date_scrape[1]\n",
    "        date_time.append([date, time_])\n",
    "        \n",
    "        \n",
    "    return date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to prepare dataframe with [ticker, date, time, headline, news, content, article_site, run_time, req_time, wait_time, soup_time, cont_time]\n",
    "# Thread Pool Processor\n",
    "\n",
    "# input is a row [ticker, pid, run_time, str_news_table] from news_table_df\n",
    "# return completed dataframe for 1 ticker (to be appended)\n",
    "# also records time to process 1 ticker and saves in list 'time_per_ticker'\n",
    "\n",
    "def ticker_to_dataframe_thr(row):\n",
    "    \n",
    "    ticker_time_start = time.perf_counter()\n",
    "    \n",
    "    \n",
    "    # convert str_news_table to html format\n",
    "    html_news_table = BeautifulSoup(row[3], 'html.parser')\n",
    "    # split into list of articles in html format\n",
    "    article_list = html_news_table.findAll('tr')\n",
    "    # get date and time\n",
    "    date_time_list = date_to_list(article_list)\n",
    "    # convert all html to str\n",
    "    article_list = [str(x) for x in article_list]\n",
    "    \n",
    "    # executor\n",
    "    if __name__ == '__main__':\n",
    "        executor = ThreadPoolExecutor(max_workers=10)\n",
    "        thread_2 = executor.map(article_details, date_time_list, article_list)\n",
    "        ticker_df = [[date, time_, news_source, headline, content, site] for date, time_, headline, news_source, content, site in thread_2]\n",
    "        ticker_df = pd.DataFrame(ticker_df, columns=['date', 'time', 'news', 'headline', 'content', 'article_site'])\n",
    "        \n",
    "        \n",
    "    ticker = row[0]\n",
    "    ticker_col = pd.Series([ticker] * len(ticker_df))\n",
    "    \n",
    "    ticker_df.insert(0, 'ticker', ticker_col)\n",
    "        \n",
    "    \n",
    "    ticker_time_end = time.perf_counter()\n",
    "    time_per_ticker_1.append(ticker_time_end - ticker_time_start)\n",
    "\n",
    "    print('Ticker complete: ', ticker)\n",
    "    \n",
    "    return ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to prepare dataframe with [ticker, date, time, headline, news, content, article_site, run_time, req_time, wait_time, soup_time, cont_time]\n",
    "# Thread Pool Processor\n",
    "\n",
    "# input is a row [ticker, pid, run_time, str_news_table] from news_table_df\n",
    "# return completed dataframe for 1 ticker (to be appended)\n",
    "# also records time to process 1 ticker and saves in list 'time_per_ticker'\n",
    "\n",
    "def ticker_to_dataframe_thrv2(row):\n",
    "    \n",
    "    ticker_time_start = time.perf_counter()\n",
    "    \n",
    "    \n",
    "    # convert str_news_table to html format\n",
    "    html_news_table = BeautifulSoup(row[3], 'html.parser')\n",
    "    # split into list of articles in html format\n",
    "    article_list = html_news_table.findAll('tr')\n",
    "    # get date and time\n",
    "    date_time_list = date_to_list(article_list)\n",
    "    # convert all html to str\n",
    "    article_list = [str(x) for x in article_list]\n",
    "    \n",
    "    # executor\n",
    "    if __name__ == '__main__':\n",
    "        ticker_df = []\n",
    "        executor = ThreadPoolExecutor(max_workers=10)\n",
    "        thread_2 = executor.map(article_details, date_time_list, article_list)\n",
    "        for i in thread_2:\n",
    "            ticker_df.append(i)\n",
    "        ticker_df = pd.DataFrame(ticker_df, columns=['date', 'time', 'news', 'headline', 'content', 'article_site'])\n",
    "        \n",
    "        \n",
    "    ticker = row[0]\n",
    "    ticker_col = pd.Series([ticker] * len(ticker_df))\n",
    "    \n",
    "    ticker_df.insert(0, 'ticker', ticker_col)\n",
    "        \n",
    "    \n",
    "    ticker_time_end = time.perf_counter()\n",
    "    time_per_ticker_1.append(ticker_time_end - ticker_time_start)\n",
    "\n",
    "    print('Ticker complete: ', ticker)\n",
    "    \n",
    "    return ticker_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run map function\n",
    "\n",
    "# define test df\n",
    "test_news_table_df = news_table_df.copy()\n",
    "test_news_table_df = test_news_table_df.iloc[0:5]\n",
    "print('Testing input')\n",
    "print(test_news_table_df)\n",
    "\n",
    "time_per_ticker_1 = []\n",
    "\n",
    "compile_start = time.perf_counter()\n",
    "\n",
    "test_art_det_df = pd.DataFrame([])\n",
    "\n",
    "test_news_table_list = test_news_table_df.values.tolist()\n",
    "check_time_start = time.perf_counter()\n",
    "mapper = list(map(ticker_to_dataframe_thr, test_news_table_list))\n",
    "check_time_end = time.perf_counter()\n",
    "for i in mapper:\n",
    "    test_art_det_df = test_art_det_df.append(i)\n",
    "\n",
    "test_art_det_df = test_art_det_df.reset_index()\n",
    "print('Time to map: ', check_time_end - check_time_start)\n",
    "    \n",
    "compile_end = time.perf_counter()\n",
    "\n",
    "print(test_art_det_df.head(10))\n",
    "print('Time taken to compile 5 tickers is: ', compile_end - compile_start, ' seconds')\n",
    "minutes = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) // 60 \n",
    "seconds = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) % 60\n",
    "print('Estimated time to compile all tickers is: ', minutes, 'minutes', seconds, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run 3 workers\n",
    "\n",
    "# define test df\n",
    "test_news_table_df = news_table_df.copy()\n",
    "test_news_table_df = test_news_table_df.iloc[0:3]\n",
    "print('Testing input')\n",
    "print(test_news_table_df)\n",
    "\n",
    "time_per_ticker_1 = []\n",
    "\n",
    "compile_start = time.perf_counter()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_news_table_list = test_news_table_df.values.tolist()\n",
    "    exe = get_reusable_executor(max_workers=3, timeout=5)\n",
    "    print('Executor initiated.')\n",
    "    process_3 = exe.map(ticker_to_dataframe_thr, test_news_table_list)\n",
    "    process_3 = [[ticker, date, time_, news, headline, content, site] for ticker, date, time_, news, headline, content, site in process_3]\n",
    "    print('Process 3 setup complete')\n",
    "    test_art_det_df = []\n",
    "    print('First dataframe converted')\n",
    "    print(process_3[0])\n",
    "    for i in process_3:\n",
    "        test_art_det_df = test_art_det_df.append(i)\n",
    "\n",
    "test_art_det_df = test_art_det_df.reset_index()\n",
    "\n",
    "compile_end = time.perf_counter()\n",
    "\n",
    "print(test_art_det_df.head(10))\n",
    "print('Time taken to compile 5 tickers is: ', compile_end - compile_start, ' seconds')\n",
    "minutes = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) // 60 \n",
    "seconds = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) % 60\n",
    "print('Estimated time to compile all tickers is: ', minutes, 'minutes', seconds, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run 10 workers\n",
    "\n",
    "# define test df\n",
    "test_news_table_df = news_table_df.copy()\n",
    "test_news_table_df = test_news_table_df.iloc[0:10]\n",
    "print('Testing input')\n",
    "print(test_news_table_df)\n",
    "\n",
    "time_per_ticker_1 = []\n",
    "\n",
    "compile_start = time.perf_counter()\n",
    "\n",
    "test_art_det_df = pd.DataFrame([])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_news_table_list = test_news_table_df.values.tolist()\n",
    "    exe = get_reusable_executor(max_workers=10, timeout=5)\n",
    "    process_3 = list(exe.map(ticker_to_dataframe_thr, test_news_table_list))\n",
    "    for i in list_of_ticker_df:\n",
    "        test_art_det_df = test_art_det_df.append(i)\n",
    "\n",
    "compile_end = time.perf_counter()\n",
    "    \n",
    "print(test_art_det_df.head(10))\n",
    "print('Time taken to compile 5 tickers is: ', compile_end - compile_start, ' seconds')\n",
    "minutes = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) // 60 \n",
    "seconds = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) % 60\n",
    "print('Estimated time to compile all tickers is: ', minutes, 'minutes', seconds, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test run 5 workers\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# define test df\n",
    "test_news_table_df = news_table_df.copy()\n",
    "test_news_table_df = test_news_table_df.iloc[0:10]\n",
    "print('Testing input')\n",
    "print(test_news_table_df)\n",
    "\n",
    "time_per_ticker_1 = []\n",
    "\n",
    "compile_start = time.perf_counter()\n",
    "\n",
    "test_art_det_df = pd.DataFrame([])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_news_table_list = test_news_table_df.values.tolist()\n",
    "    exe = get_reusable_executor(max_workers=5, timeout=5)\n",
    "    process_3 = exe.map(ticker_to_dataframe_thr, test_news_table_list)\n",
    "    list_of_ticker_df = list(process_3)\n",
    "    for i in list_of_ticker_df:\n",
    "        test_art_det_df = test_art_det_df.append(i)\n",
    "\n",
    "compile_end = time.perf_counter()\n",
    "    \n",
    "print(test_art_det_df.head(10))\n",
    "print('Time taken to compile 5 tickers is: ', compile_end - compile_start, ' seconds')\n",
    "minutes = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) // 60 \n",
    "seconds = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) % 60\n",
    "print('Estimated time to compile all tickers is: ', minutes, 'minutes', seconds, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "   0  1  2\n",
      "0  0  1  2\n",
      "1  0  1  2\n",
      "2  0  1  2\n",
      "3  0  1  2\n",
      "4  0  1  2\n",
      "0  2  3  4\n",
      "1  2  3  4\n",
      "2  2  3  4\n",
      "3  2  3  4\n",
      "4  2  3  4\n",
      "0  3  4  5\n",
      "1  3  4  5\n",
      "2  3  4  5\n",
      "3  3  4  5\n",
      "4  3  4  5\n",
      "0  0  1  2\n",
      "1  0  1  2\n",
      "2  0  1  2\n",
      "3  0  1  2\n",
      "4  0  1  2\n",
      "0  4  5  6\n",
      "1  4  5  6\n",
      "2  4  5  6\n",
      "3  4  5  6\n",
      "4  4  5  6\n"
     ]
    }
   ],
   "source": [
    "def do_something_else(x):\n",
    "    time.sleep(x/2)\n",
    "    print([x-1, x, x+1])\n",
    "    return [x-1, x, x+1]\n",
    "\n",
    "def do_something(row):\n",
    "    for x in row:\n",
    "        do_something_else(x)\n",
    "        \n",
    "    if __name__ == '__main__':\n",
    "        executor = ThreadPoolExecutor()\n",
    "        thread = executor.map(do_something_else, row)\n",
    "        df = pd.DataFrame([a for a in thread])\n",
    "        for i in thread:\n",
    "            print(i)\n",
    "            #df = df.append(i)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "sample = [[1, 1, 1, 1, 1], [3, 3, 3, 3, 3], [4, 4, 4, 4, 4], [1, 1, 1, 1, 1], [5, 5, 5, 5, 5]]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    exe = get_reusable_executor(max_workers=3, timeout=2)\n",
    "    p = exe.map(do_something, sample)\n",
    "    test_output = pd.DataFrame([])\n",
    "    test_list = list(p)\n",
    "    print('Here')\n",
    "    for k in test_list:\n",
    "        test_output = test_output.append(k)\n",
    "    \n",
    "        \n",
    "    #test_output = [a for a in p]\n",
    "    #test_df = pd.DataFrame([])\n",
    "    #for x in test_output:\n",
    "    #   test_df = test_df.append(test_output)\n",
    "\n",
    "#test_output = test_output.append([1, 2, 3])\n",
    "print(test_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
