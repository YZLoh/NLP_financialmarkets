{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Collect-S&amp;P-500-Companies\" data-toc-modified-id=\"Collect-S&amp;P-500-Companies-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Collect S&amp;P 500 Companies</a></span></li><li><span><a href=\"#Example-code\" data-toc-modified-id=\"Example-code-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Example code</a></span></li><li><span><a href=\"#Stock-Prices\" data-toc-modified-id=\"Stock-Prices-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Stock Prices</a></span></li><li><span><a href=\"#Calculate-Correlation\" data-toc-modified-id=\"Calculate-Correlation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Calculate Correlation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:30:59.804887Z",
     "start_time": "2021-02-13T02:30:59.297351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect S&P 500 Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:31:00.240968Z",
     "start_time": "2021-02-13T02:30:59.805895Z"
    }
   },
   "outputs": [],
   "source": [
    "table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "sandp_df = table[0]\n",
    "\n",
    "#sandp_df.to_csv('data/S&P500-Info.csv')\n",
    "#sandp_df.to_csv(\"data/S&P500-Symbols.csv\", columns=['Symbol'])\n",
    "\n",
    "#https://medium.com/wealthy-bytes/5-lines-of-python-to-automate-getting-the-s-p-500-95a632e5e567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:31:00.254870Z",
     "start_time": "2021-02-13T02:31:00.242900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>SEC filings</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date first added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>reports</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>St. Paul, Minnesota</td>\n",
       "      <td>1976-08-09</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>reports</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1964-03-31</td>\n",
       "      <td>1800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie Inc.</td>\n",
       "      <td>reports</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABMD</td>\n",
       "      <td>Abiomed</td>\n",
       "      <td>reports</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>Danvers, Massachusetts</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>815094</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>reports</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>1467373</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol             Security SEC filings             GICS Sector  \\\n",
       "0    MMM           3M Company     reports             Industrials   \n",
       "1    ABT  Abbott Laboratories     reports             Health Care   \n",
       "2   ABBV          AbbVie Inc.     reports             Health Care   \n",
       "3   ABMD              Abiomed     reports             Health Care   \n",
       "4    ACN            Accenture     reports  Information Technology   \n",
       "\n",
       "                GICS Sub-Industry    Headquarters Location Date first added  \\\n",
       "0        Industrial Conglomerates      St. Paul, Minnesota       1976-08-09   \n",
       "1           Health Care Equipment  North Chicago, Illinois       1964-03-31   \n",
       "2                 Pharmaceuticals  North Chicago, Illinois       2012-12-31   \n",
       "3           Health Care Equipment   Danvers, Massachusetts       2018-05-31   \n",
       "4  IT Consulting & Other Services          Dublin, Ireland       2011-07-06   \n",
       "\n",
       "       CIK      Founded  \n",
       "0    66740         1902  \n",
       "1     1800         1888  \n",
       "2  1551152  2013 (1888)  \n",
       "3   815094         1981  \n",
       "4  1467373         1989  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandp_df.head(5)\n",
    "# so the symbol is the same as the corresponding stock ticker. \n",
    "# It will be used for parsing news results that reference the company that made the headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example code\n",
    "\n",
    "Taken from https://towardsdatascience.com/sentiment-analysis-of-stocks-from-financial-news-using-python-82ebdcefb638"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:34:22.676103Z",
     "start_time": "2021-02-13T02:32:07.975282Z"
    }
   },
   "outputs": [],
   "source": [
    "finwiz_url = 'https://finviz.com/quote.ashx?t='\n",
    "\n",
    "news_tables = {}\n",
    "tickers = sandp_df['Symbol']\n",
    "\n",
    "for ticker in tickers:\n",
    "    url = finwiz_url + ticker\n",
    "    req = Request(url=url, headers={'user-agent': 'my-app/0.0.1'}) \n",
    "    try:\n",
    "        response = urlopen(req)   \n",
    "        html = BeautifulSoup(response)  # Read the contents of the file into 'html'\n",
    "        news_table = html.find(id='news-table') # Find 'news-table' in the Soup and load it into 'news_table'\n",
    "        news_tables[ticker] = news_table # Add the table to our dictionary\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to generate [ticker, date, time, headline, news_source] given a article\n",
    "def article_summary(html_article):\n",
    "    # read the text from each article into text\n",
    "        # get text from a only\n",
    "        headline = html_article.a.get_text() \n",
    "        # splite text in the td tag into a list \n",
    "        date_scrape = html_article.td.text.split()\n",
    "        \n",
    "        # get news media company\n",
    "        news = html_article.span.get_text()\n",
    "        \n",
    "        global date\n",
    "        # if the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "            \n",
    "        # else load 'date' as the 1st element and 'time' as the second    \n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "        \n",
    "        # Return [ticker, date, time, headline, news source] to be appended\n",
    "        return [date, time, headline, news]\n",
    "\n",
    "    \n",
    "# define function to generate article content (in a list) given a article\n",
    "# only works with articles from yahoo.finance right now\n",
    "def article_content(html_article):\n",
    "    # get the link to the full article\n",
    "    link = html_article.find('a').get('href')\n",
    "    content = 'empty string'\n",
    "    # check if link leads to yahoo.finance\n",
    "    if urlparse(link).netloc == 'finance.yahoo.com':\n",
    "        try:\n",
    "            # request from yahoo.finance\n",
    "            req_art = Request(url=link, headers={'user-agent':'my-app/0.0.1'})\n",
    "            response_art = urlopen(req_art)\n",
    "            html_art = BeautifulSoup(response_art)\n",
    "            # get the article content\n",
    "            content = html_art.find(class_='caas-body').get_text()\n",
    "        except:\n",
    "            print('Error following article link: ', link)\n",
    "    else:\n",
    "        return [content, urlparse(link).netloc]\n",
    "    return [content]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to prepare data with ticker, date, time, headline, news, content\n",
    "def prepare_data(html_dict):\n",
    "    \n",
    "    pid = os.get_pid()\n",
    "    \n",
    "    # define where data will be saved\n",
    "    global parsed_news\n",
    "    global rejected_sites\n",
    "    parsed_news = []\n",
    "    rejected_sites = []\n",
    "    \n",
    "    for file_name, news_str in html_dict.items():\n",
    "        # soupify str\n",
    "        news_html = BeautifulSoup(news_str, 'html.parser')\n",
    "        # iterate through all tr tags in 'news_table'\n",
    "        for x in news_html.findAll('tr'):\n",
    "            # article content and rejected site (if applicable) (use to expand available sites in the future)\n",
    "            art_content = article_content(x)\n",
    "            ticker = file_name.split('_')[0]\n",
    "            # combine article summary and article content into 1 row of info about the article\n",
    "            art_row = [ticker] + article_summary(x) + [art_content[0]]\n",
    "            # append relevant info to the 'parsed_news' list\n",
    "            parsed_news.append(art_row)\n",
    "            if len(art_content) > 1:\n",
    "                rejected_sites.append(art_content[1])\n",
    "        print('Done parsing through: ', ticker)\n",
    "    return pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tr><td align=\"right\" style=\"white-space:nowrap\" width=\"130\">Feb-14-21 09:27AM  </td><td align=\"left\"><div class=\"news-link-container\"><div class=\"news-link-left\"><a class=\"tab-link-news\" href=\"https://www.fool.com/investing/2021/02/14/these-3-dividend-stocks-are-too-cheap-to-ignore/?source=eptyholnk0000202&amp;utm_source=yahoo-host&amp;utm_medium=feed&amp;utm_campaign=article\" target=\"_blank\">These 3 Dividend Stocks Are Too Cheap to Ignore</a></div><div class=\"news-link-right\"><span style=\"color:#aa6dc0;font-size:9px\"> Motley Fool</span></div></div></td></tr>\n"
     ]
    }
   ],
   "source": [
    "#print(test_news_tables['MMM'])\n",
    "print(pd.DataFrame.from_dict(test_news_tables, orient='index').iloc[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor ID:  Processing Complete\n",
      "Percentage decrease in articles is:  0.31666666666666665\n",
      "   ticker       date     time  \\\n",
      "1     MMM  Feb-12-21  05:12PM   \n",
      "5     MMM  Feb-09-21  02:35PM   \n",
      "6     MMM  Feb-09-21  11:07AM   \n",
      "7     MMM  Feb-09-21  11:07AM   \n",
      "9     MMM  Feb-06-21  09:12AM   \n",
      "12    MMM  Feb-06-21  02:53AM   \n",
      "13    MMM  Feb-04-21  10:05AM   \n",
      "14    MMM  Feb-04-21  08:25AM   \n",
      "15    MMM  Feb-02-21  07:53PM   \n",
      "16    MMM  Feb-02-21  07:15PM   \n",
      "\n",
      "                                             headline              news  \\\n",
      "1     First Eagle Investment's Top 4th-Quarter Trades     GuruFocus.com   \n",
      "5               3M Announces Upcoming Investor Events       PR Newswire   \n",
      "6   New 3M Polisher ST reduces the number of bioph...       PR Newswire   \n",
      "7   New 3M Polisher ST reduces the number of bioph...         CNW Group   \n",
      "9                30 Dividend Kings of 2021 (Part III)    Insider Monkey   \n",
      "12  Income Investors Should Know That 3M Company (...   Simply Wall St.   \n",
      "13  3M and Discovery Education Open Call for Entri...       PR Newswire   \n",
      "14  AIRO.LIFE secures Trademark in the United Stat...         CNW Group   \n",
      "15   Dow Inc CEO Jim Fitterling elected to 3M's board           Reuters   \n",
      "16  3M Board Declares Increase to First Quarter 20...       PR Newswire   \n",
      "\n",
      "                                              content  \n",
      "1   - By Graham GriffinFirst Eagle Investment (Tra...  \n",
      "5   ST. PAUL, Minn., Feb. 9, 2021 /PRNewswire/ -- ...  \n",
      "6   ST. PAUL, Minn., Feb. 9, 2021 /PRNewswire/ -- ...  \n",
      "7   ST. PAUL, Minn., Feb. 9, 2021 /CNW/ -- 3M Heal...  \n",
      "9   This is the last part of Insider Monkey's thre...  \n",
      "12  It looks like 3M Company (NYSE:MMM) is about t...  \n",
      "13  Annual 3M Young Scientist Challenge Offers Uni...  \n",
      "14  TORONTO, Feb. 4, 2021 /CNW/ - AIRO.LIFE is ple...  \n",
      "15  (Reuters) - 3M Co, which makes N95 face masks,...  \n",
      "16  ST. PAUL, Minn., Feb. 2, 2021 /PRNewswire/ -- ...  \n",
      "['www.fool.com', 'www.fool.com', 'www.fool.com', 'www.barrons.com', 'www.fool.com', 'www.barrons.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.barrons.com', 'www.fool.com', 'www.barrons.com', 'www.investors.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.fool.com']\n",
      "Process Pool Executor finished in:  178.13286640000024  seconds\n"
     ]
    }
   ],
   "source": [
    "# Process Pool Executor\n",
    "\n",
    "from loky import get_reusable_executor\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "start_test1 = time.perf_counter()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_news_tables = news_tables.copy()\n",
    "    test_news_tables = dict(zip(['MMM', 'ABT', 'ABBV'], [str(test_news_tables[x]) for x in ['MMM', 'ABT', 'ABBV']]))\n",
    "    executor = get_reusable_executor(max_workers=4, timeout=2)\n",
    "    p = executor.submit(prepare_data, test_news_tables)\n",
    "    print('Processor ID: ', p.result())\n",
    "        \n",
    "        \n",
    "columns = ['ticker', 'date', 'time', 'headline', 'news', 'content']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_news_updated'\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "ori_len = len(parsed_news_updated)\n",
    "# remove articles whose contents are not available\n",
    "parsed_news_updated_cont = parsed_news_updated.copy()\n",
    "parsed_news_updated_cont = parsed_news_updated_cont.loc[parsed_news_updated_cont['content'] != 'empty string']\n",
    "\n",
    "parsed_news_updated = parsed_news_updated.drop('content', axis=1)\n",
    "\n",
    "print('Percentage decrease in articles is: ', (ori_len - len(parsed_news_updated_cont))/ori_len)\n",
    "\n",
    "\n",
    "print(parsed_news_updated_cont.head(10))\n",
    "print(rejected_sites[:20])\n",
    "\n",
    "end_test1 = time.perf_counter()\n",
    "\n",
    "print('Process Pool Executor finished in: ', end_test1 - start_test1, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done parsing through:  MMM\n",
      "Done parsing through:  ABT\n",
      "Done parsing through:  ABBV\n",
      "Processing Complete\n",
      "Percentage decrease in articles is:  0.31666666666666665\n",
      "   ticker       date     time  \\\n",
      "1     MMM  Feb-12-21  05:12PM   \n",
      "5     MMM  Feb-09-21  02:35PM   \n",
      "6     MMM  Feb-09-21  11:07AM   \n",
      "7     MMM  Feb-09-21  11:07AM   \n",
      "9     MMM  Feb-06-21  09:12AM   \n",
      "12    MMM  Feb-06-21  02:53AM   \n",
      "13    MMM  Feb-04-21  10:05AM   \n",
      "14    MMM  Feb-04-21  08:25AM   \n",
      "15    MMM  Feb-02-21  07:53PM   \n",
      "16    MMM  Feb-02-21  07:15PM   \n",
      "\n",
      "                                             headline              news  \\\n",
      "1     First Eagle Investment's Top 4th-Quarter Trades     GuruFocus.com   \n",
      "5               3M Announces Upcoming Investor Events       PR Newswire   \n",
      "6   New 3M Polisher ST reduces the number of bioph...       PR Newswire   \n",
      "7   New 3M Polisher ST reduces the number of bioph...         CNW Group   \n",
      "9                30 Dividend Kings of 2021 (Part III)    Insider Monkey   \n",
      "12  Income Investors Should Know That 3M Company (...   Simply Wall St.   \n",
      "13  3M and Discovery Education Open Call for Entri...       PR Newswire   \n",
      "14  AIRO.LIFE secures Trademark in the United Stat...         CNW Group   \n",
      "15   Dow Inc CEO Jim Fitterling elected to 3M's board           Reuters   \n",
      "16  3M Board Declares Increase to First Quarter 20...       PR Newswire   \n",
      "\n",
      "                                              content  \n",
      "1   - By Graham GriffinFirst Eagle Investment (Tra...  \n",
      "5   ST. PAUL, Minn., Feb. 9, 2021 /PRNewswire/ -- ...  \n",
      "6   ST. PAUL, Minn., Feb. 9, 2021 /PRNewswire/ -- ...  \n",
      "7   ST. PAUL, Minn., Feb. 9, 2021 /CNW/ -- 3M Heal...  \n",
      "9   This is the last part of Insider Monkey's thre...  \n",
      "12  It looks like 3M Company (NYSE:MMM) is about t...  \n",
      "13  Annual 3M Young Scientist Challenge Offers Uni...  \n",
      "14  TORONTO, Feb. 4, 2021 /CNW/ - AIRO.LIFE is ple...  \n",
      "15  (Reuters) - 3M Co, which makes N95 face masks,...  \n",
      "16  ST. PAUL, Minn., Feb. 2, 2021 /PRNewswire/ -- ...  \n",
      "['www.fool.com', 'www.fool.com', 'www.fool.com', 'www.barrons.com', 'www.fool.com', 'www.barrons.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.barrons.com', 'www.fool.com', 'www.barrons.com', 'www.investors.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.fool.com']\n",
      "Thread Pool Executor finished in:  181.81914359999973  seconds\n"
     ]
    }
   ],
   "source": [
    "# Thread Pool Executor\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "start_test1 = time.perf_counter()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_news_tables = news_tables.copy()\n",
    "    test_news_tables = dict(zip(['MMM', 'ABT', 'ABBV'], [str(test_news_tables[x]) for x in ['MMM', 'ABT', 'ABBV']]))\n",
    "    executor = ThreadPoolExecutor()\n",
    "    p = executor.submit(prepare_data, test_news_tables)\n",
    "    print(p.result())\n",
    "        \n",
    "        \n",
    "columns = ['ticker', 'date', 'time', 'headline', 'news', 'content']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_news_updated'\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "ori_len = len(parsed_news_updated)\n",
    "# remove articles whose contents are not available\n",
    "parsed_news_updated_cont = parsed_news_updated.copy()\n",
    "parsed_news_updated_cont = parsed_news_updated_cont.loc[parsed_news_updated_cont['content'] != 'empty string']\n",
    "\n",
    "parsed_news_updated = parsed_news_updated.drop('content', axis=1)\n",
    "\n",
    "print('Percentage decrease in articles is: ', (ori_len - len(parsed_news_updated_cont))/ori_len)\n",
    "\n",
    "\n",
    "print(parsed_news_updated_cont.head(10))\n",
    "print(rejected_sites[:20])\n",
    "\n",
    "end_test1 = time.perf_counter()\n",
    "\n",
    "print('Thread Pool Executor finished in: ', end_test1 - start_test1, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done parsing through:  MMM\n",
      "Done parsing through:  ABT\n",
      "Done parsing through:  ABBV\n",
      "Processing Complete\n",
      "Percentage decrease in articles is:  0.31666666666666665\n",
      "   ticker       date     time  \\\n",
      "1     MMM  Feb-12-21  05:12PM   \n",
      "5     MMM  Feb-09-21  02:35PM   \n",
      "6     MMM  Feb-09-21  11:07AM   \n",
      "7     MMM  Feb-09-21  11:07AM   \n",
      "9     MMM  Feb-06-21  09:12AM   \n",
      "12    MMM  Feb-06-21  02:53AM   \n",
      "13    MMM  Feb-04-21  10:05AM   \n",
      "14    MMM  Feb-04-21  08:25AM   \n",
      "15    MMM  Feb-02-21  07:53PM   \n",
      "16    MMM  Feb-02-21  07:15PM   \n",
      "\n",
      "                                             headline              news  \\\n",
      "1     First Eagle Investment's Top 4th-Quarter Trades     GuruFocus.com   \n",
      "5               3M Announces Upcoming Investor Events       PR Newswire   \n",
      "6   New 3M Polisher ST reduces the number of bioph...       PR Newswire   \n",
      "7   New 3M Polisher ST reduces the number of bioph...         CNW Group   \n",
      "9                30 Dividend Kings of 2021 (Part III)    Insider Monkey   \n",
      "12  Income Investors Should Know That 3M Company (...   Simply Wall St.   \n",
      "13  3M and Discovery Education Open Call for Entri...       PR Newswire   \n",
      "14  AIRO.LIFE secures Trademark in the United Stat...         CNW Group   \n",
      "15   Dow Inc CEO Jim Fitterling elected to 3M's board           Reuters   \n",
      "16  3M Board Declares Increase to First Quarter 20...       PR Newswire   \n",
      "\n",
      "                                              content  \n",
      "1   - By Graham GriffinFirst Eagle Investment (Tra...  \n",
      "5   ST. PAUL, Minn., Feb. 9, 2021 /PRNewswire/ -- ...  \n",
      "6   ST. PAUL, Minn., Feb. 9, 2021 /PRNewswire/ -- ...  \n",
      "7   ST. PAUL, Minn., Feb. 9, 2021 /CNW/ -- 3M Heal...  \n",
      "9   This is the last part of Insider Monkey's thre...  \n",
      "12  It looks like 3M Company (NYSE:MMM) is about t...  \n",
      "13  Annual 3M Young Scientist Challenge Offers Uni...  \n",
      "14  TORONTO, Feb. 4, 2021 /CNW/ - AIRO.LIFE is ple...  \n",
      "15  (Reuters) - 3M Co, which makes N95 face masks,...  \n",
      "16  ST. PAUL, Minn., Feb. 2, 2021 /PRNewswire/ -- ...  \n",
      "['www.fool.com', 'www.fool.com', 'www.fool.com', 'www.barrons.com', 'www.fool.com', 'www.barrons.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.barrons.com', 'www.fool.com', 'www.barrons.com', 'www.investors.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.fool.com', 'www.fool.com']\n",
      "Synchronous test finished in:  157.51485450000018  seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "start_test2 = time.perf_counter()\n",
    "\n",
    "\n",
    "test_news_tables = news_tables.copy()\n",
    "test_news_tables = dict(zip(['MMM', 'ABT', 'ABBV'], [str(test_news_tables[x]) for x in ['MMM', 'ABT', 'ABBV']]))\n",
    "print(prepare_data(test_news_tables))\n",
    "\n",
    "# Set column names\n",
    "columns = ['ticker', 'date', 'time', 'headline', 'news', 'content']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "ori_len = len(parsed_news_updated)\n",
    "# remove articles whose contents are not available\n",
    "parsed_news_updated_content = parsed_news_updated.copy()\n",
    "parsed_news_updated_content = parsed_news_updated_content.loc[parsed_news_updated['content'] != 'empty string']\n",
    "parsed_news_updated = parsed_news_updated.drop('content', axis=1)\n",
    "print('Percentage decrease in articles is: ', (ori_len - len(parsed_news_updated_content))/ori_len)\n",
    "\n",
    "\n",
    "print(parsed_news_updated_content.head(10))\n",
    "print(rejected_sites[:20])\n",
    "\n",
    "\n",
    "end_test2 = time.perf_counter()\n",
    "\n",
    "print('Synchronous test finished in: ', end_test2 - start_test2, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:34:38.201871Z",
     "start_time": "2021-02-13T02:34:34.857909Z"
    }
   },
   "outputs": [],
   "source": [
    "parsed_news = []\n",
    "rejected_sites = []\n",
    "\n",
    "# Iterate through the news\n",
    "for file_name, news_table in news_tables.items():\n",
    "    # Iterate through all tr tags in 'news_table'\n",
    "    for x in news_table.findAll('tr'):\n",
    "        art_content = article_content(x)\n",
    "        # combine article summary and article content into 1 row of info about the article\n",
    "        article_row = article_summary(x) + [art_content[0]]\n",
    "        # Append relevant info as a list to the 'parsed_news' list\n",
    "        parsed_news.append(article_row)\n",
    "        rejected_sites.append(art_content[1])\n",
    "\n",
    "# Set column names\n",
    "columns = ['ticker', 'date', 'time', 'headline', 'news', 'content']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "ori_len = len(parsed_news_updated)\n",
    "# remove articles whose contents are not available\n",
    "parsed_news_updated_content = parsed_news_updated.copy()\n",
    "parsed_news_updated_content = parsed_news_updated_content.loc[parsed_news_updated['content'] != 'empty string']\n",
    "parsed_news_updated = parsed_news_updated.drop('content', axis=1)\n",
    "print('Percentage decrease in articles is: ', (ori_len - len(parsed_news_updated_content))/ori_len)\n",
    "parsed_news_updated_content.head(10)\n",
    "\n",
    "#parsed_news_updated does not have the content column and has full number of articles (100 per ticker)\n",
    "#parsed_news_updated_content has content column and is shortened to remove all rows with no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of headlines produced by each news source and remove news sources with <median headlines\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "# generate series of counts\n",
    "news_count = parsed_news_updated['news'].value_counts()\n",
    "# set minimum count = median\n",
    "cutoff_point = news_count.median()\n",
    "# append count of news to dataframe\n",
    "parsed_news_updated = parsed_news_updated.merge(news_count, left_on='news', right_index=True)\n",
    "parsed_news_updated = parsed_news_updated.drop('news_x', axis=1).rename({'news_y' : 'count'}, axis=1)\n",
    "# remove news which have count < cutoff_point\n",
    "parsed_news_updated = parsed_news_updated.loc[parsed_news_updated['count'] > cutoff_point]\n",
    "parsed_news_updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:34:48.450433Z",
     "start_time": "2021-02-13T02:34:41.894994Z"
    }
   },
   "outputs": [],
   "source": [
    "#need to tokenize each words within the headlines to improve the sentiment score.\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def regex(x):\n",
    "    special_chars_p = \"[.®'&$’\\\"\\-()#@!?/:]\"\n",
    "    s1 = re.sub(special_chars_p, '', x)  \n",
    "    return(s1)\n",
    "\n",
    "parsed_news_updated['headline'] = parsed_news_updated['headline'].apply(regex)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.lower().split()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "parsed_news_updated_stem = parsed_news_updated.copy()\n",
    "parsed_news_updated_stem['headline'] = parsed_news_updated_stem['headline'].apply(stem_sentences)\n",
    "\n",
    "stop=stopwords.words('english')\n",
    "\n",
    "parsed_news_updated['headline'].apply(lambda x: [item for item in x if item not in stop])\n",
    "parsed_news_updated_stem['headline'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "parsed_news_updated['headline'] = parsed_news_updated['headline'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) \n",
    "parsed_news_updated_stem['headline'] = parsed_news_updated_stem['headline'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) \n",
    "parsed_news_updated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:34:56.695664Z",
     "start_time": "2021-02-13T02:34:50.966410Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NLTK VADER for sentiment analysis (unstem)\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Instantiate the sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Iterate through the headlines and get the polarity scores using vader\n",
    "scores = parsed_news_updated['headline'].apply(vader.polarity_scores).tolist()\n",
    "\n",
    "# Convert the 'scores' list of dicts into a DataFrame\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# Join the DataFrames of the news and the list of dicts\n",
    "parsed_and_scored_news = parsed_news_updated.join(scores_df, rsuffix='_right')\n",
    "\n",
    "# Convert the date column from string to datetime\n",
    "parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.date\n",
    "\n",
    "parsed_and_scored_news.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK VADER for sentiment analysis (stem)\n",
    "\n",
    "# Instantiate the sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Iterate through the headlines and get the polarity scores using vader\n",
    "scores_stem = parsed_news_updated_stem['headline'].apply(vader.polarity_scores).tolist()\n",
    "\n",
    "# Convert the 'scores' list of dicts into a DataFrame\n",
    "scores_stem_df = pd.DataFrame(scores_stem)\n",
    "\n",
    "# Join the DataFrames of the news and the list of dicts\n",
    "parsed_and_scored_news_stem = parsed_news_updated_stem.join(scores_stem_df, rsuffix='_right')\n",
    "\n",
    "# Convert the date column from string to datetime\n",
    "parsed_and_scored_news_stem['date'] = pd.to_datetime(parsed_and_scored_news_stem.date).dt.date\n",
    "\n",
    "parsed_and_scored_news_stem.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:35:12.704918Z",
     "start_time": "2021-02-13T02:35:12.633876Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get S&P 500 prices\n",
    "# source: https://www.spglobal.com/spdji/en/indices/equity/sp-500/#overview\n",
    "\n",
    "df_sp = pd.read_csv('data/S&P500_5years.csv', usecols=[0,1]) # Use only first 2 columns\n",
    "df_sp.columns = ['date', 'price']\n",
    "df_sp['date'] = pd.to_datetime(df_sp['date'])\n",
    "df_sp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:36:17.050503Z",
     "start_time": "2021-02-13T02:35:14.178417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get S&P 500 individual stock prices\n",
    "\n",
    "# Create a function to get stock price given a ticker \n",
    "def get_stock_price(ticker, start, end):\n",
    "    '''Get prices of a stock in a given period.\n",
    "    \n",
    "    Args:\n",
    "        ticker (str): ticker of a company \n",
    "        start (str): date in format of 'YYYY-MM-DD'\n",
    "        end (str): date in format of 'YYYY-MM-DD'\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing open, high, low, close, volume, dividends, stock splits\n",
    "    '''\n",
    "    import yfinance as yf\n",
    "    \n",
    "    ticker = yf.Ticker(ticker)\n",
    "    data = ticker.history(start=start, end=end)\n",
    "    data.reset_index(level=0, inplace=True)\n",
    "    return data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate merged dataframe (merged by date)\n",
    "# columns: ['ticker', 'date', 'time', 'headline', 'news', 'neg', 'neu', 'pos', 'compound', 'open', 'close', 'change']\n",
    "# scored dataframe should be the input (not sure if works for scored sentiment other than Vader)\n",
    "\n",
    "\n",
    "def generate_final_df(scored_df):\n",
    "    # Get a list of 505 stocks from S&P 500\n",
    "    sp500 = sandp_df['Symbol'].unique()\n",
    "    start = scored_df['date'].min()\n",
    "    end = scored_df['date'].max()\n",
    "    \n",
    "    # Iterate through each stock to get price\n",
    "    df_stock = pd.DataFrame()\n",
    "    for ticker in sp500:\n",
    "        data = get_stock_price(ticker, start, end)\n",
    "        data['ticker'] = ticker\n",
    "        df_stock = pd.concat([df_stock, data], axis=0)\n",
    "        \n",
    "    # Change all columns names to lowercase  \n",
    "    df_stock.columns = df_stock.columns.str.lower()\n",
    "    \n",
    "    # Convert timestamp to date\n",
    "    df_stock['date'] = df_stock['date'].apply(datetime.date)\n",
    "    \n",
    "    # Reset index\n",
    "    df_stock.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Merge stock price info and sentiment scores\n",
    "    df_merged = scored_df.merge(df_stock.loc[:, ['date', 'ticker', 'open', 'close']], on=['date', 'ticker'])\n",
    "    # Add column: price change\n",
    "    df_merged['change'] = df_merged['close'] - df_merged['open']\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final df for unstem and stem\n",
    "\n",
    "df_final_unstem = generate_final_df(parsed_and_scored_news)\n",
    "df_final_stem = generate_final_df(parsed_and_scored_news_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:37:43.967232Z",
     "start_time": "2021-02-13T02:37:43.917201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate pearson correlation coef between sentiment score and price for each news media\n",
    "scores_close_unstem = df_final_unstem.groupby('news')[['compound', 'close']].corr().unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_close_stem = df_final_stem.groupby('news')[['compound', 'close']].corr().unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_change_unstem = df_final_unstem.groupby('news')[['compound', 'change']].corr().unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_change_stem = df_final_stem.groupby('news')[['compound', 'change']].corr().unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/28988627/pandas-correlation-groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate spearman correlation coef between sentiment score and price for each news media\n",
    "scores_close_unstem = df_final_unstem.groupby('news')[['compound', 'close']].corr(method='spearman').unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_close_stem = df_final_stem.groupby('news')[['compound', 'close']].corr(method='spearman').unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_change_unstem = df_final_unstem.groupby('news')[['compound', 'change']].corr(method='spearman').unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_change_stem = df_final_stem.groupby('news')[['compound', 'change']].corr(method='spearman').unstack().iloc[:, 1].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson's Correlation Coefficient as Dataframe\n",
    "pearson_corr = pd.DataFrame({'variable' : ['close', 'change'], 'unstem' : [df_final_unstem[['compound', 'close']].corr().iloc[0,1], df_final_unstem[['compound', 'change']].corr().iloc[0,1]], 'stem' : [df_final_stem[['compound', 'close']].corr().iloc[0,1], df_final_stem[['compound', 'change']].corr().iloc[0,1]]})\n",
    "pearson_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman's rank correlation\n",
    "spearman_corr = pd.DataFrame({'variable' : ['close', 'change'], 'unstem' : [df_final_unstem[['compound', 'close']].corr(method='spearman').iloc[0,1], df_final_unstem[['compound', 'change']].corr(method='spearman').iloc[0,1]], 'stem' : [df_final_stem[['compound', 'close']].corr(method='spearman').iloc[0,1], df_final_stem[['compound', 'change']].corr(method='spearman').iloc[0,1]]})\n",
    "spearman_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vader Sentiment Analysis\n",
    "\n",
    "Performed Pearson's Correlation Coefficient comparisons between compound sentiment and (1) closing price (2) change in price (closing price - opening price). Computed the Spearman rank correlation coefficient as well.\n",
    "\n",
    "Compared effect of stemming and not stemming words on Pearson's and Spearman's correlation coefficient.\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Pearson:\n",
    "Correlation between both (1) and (2) is negligible (<1%) without stemming. Stemming appears to improve correlation, but correlation is still very small (<2%)\n",
    "\n",
    "Spearman:\n",
    "Correlation for both (1) and (2) is still small, but better than Pearson. Stemming has inconsistent results, slightly lowering (2) but increasing (1)\n",
    "\n",
    "Overall, Vader sentiment analysis produces very weak correlation with both (1) and (2). Try with other models.\n",
    "\n",
    "Removing headlines from news sources with less articles:\n",
    "Decreased correlation. Most of the correlations are negative but close to 0. This suggests that news sources with lower article counts tend to predict the price movements more accurately than the news sources that post more often.\n",
    "\n",
    "\n",
    "Note: \n",
    "\n",
    "1. Might be helpful to determine the most relevant news sources by taking highly correlated news sources with instances of more than 20. Limiting the data might increase correlation. Use test set to evaluate if using this method.\n",
    "\n",
    "2. Doing linear regression on neg, neu and pos score might produce interesting results.\n",
    "\n",
    "3. Vader sentiment scores might be the problem. Sentiment scores of some sample headlines was observed and Vader had many false negatives. Try using article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:38:23.792228Z",
     "start_time": "2021-02-13T02:38:23.778225Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final_unstem.loc[df_final_unstem['news']==' The Telegraph', ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. Take note of changes in the composition of S&P 500."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
