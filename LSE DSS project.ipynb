{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Collect-S&amp;P-500-Companies\" data-toc-modified-id=\"Collect-S&amp;P-500-Companies-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Collect S&amp;P 500 Companies</a></span></li><li><span><a href=\"#Example-code\" data-toc-modified-id=\"Example-code-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Example code</a></span></li><li><span><a href=\"#Stock-Prices\" data-toc-modified-id=\"Stock-Prices-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Stock Prices</a></span></li><li><span><a href=\"#Calculate-Correlation\" data-toc-modified-id=\"Calculate-Correlation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Calculate Correlation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:30:59.804887Z",
     "start_time": "2021-02-13T02:30:59.297351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect S&P 500 Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:31:00.240968Z",
     "start_time": "2021-02-13T02:30:59.805895Z"
    }
   },
   "outputs": [],
   "source": [
    "table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "sandp_df = table[0]\n",
    "\n",
    "#sandp_df.to_csv('data/S&P500-Info.csv')\n",
    "#sandp_df.to_csv(\"data/S&P500-Symbols.csv\", columns=['Symbol'])\n",
    "\n",
    "#https://medium.com/wealthy-bytes/5-lines-of-python-to-automate-getting-the-s-p-500-95a632e5e567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:31:00.254870Z",
     "start_time": "2021-02-13T02:31:00.242900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>SEC filings</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date first added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>reports</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>St. Paul, Minnesota</td>\n",
       "      <td>1976-08-09</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>reports</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1964-03-31</td>\n",
       "      <td>1800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie Inc.</td>\n",
       "      <td>reports</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABMD</td>\n",
       "      <td>Abiomed</td>\n",
       "      <td>reports</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>Danvers, Massachusetts</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>815094</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>reports</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>1467373</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol             Security SEC filings             GICS Sector  \\\n",
       "0    MMM           3M Company     reports             Industrials   \n",
       "1    ABT  Abbott Laboratories     reports             Health Care   \n",
       "2   ABBV          AbbVie Inc.     reports             Health Care   \n",
       "3   ABMD              Abiomed     reports             Health Care   \n",
       "4    ACN            Accenture     reports  Information Technology   \n",
       "\n",
       "                GICS Sub-Industry    Headquarters Location Date first added  \\\n",
       "0        Industrial Conglomerates      St. Paul, Minnesota       1976-08-09   \n",
       "1           Health Care Equipment  North Chicago, Illinois       1964-03-31   \n",
       "2                 Pharmaceuticals  North Chicago, Illinois       2012-12-31   \n",
       "3           Health Care Equipment   Danvers, Massachusetts       2018-05-31   \n",
       "4  IT Consulting & Other Services          Dublin, Ireland       2011-07-06   \n",
       "\n",
       "       CIK      Founded  \n",
       "0    66740         1902  \n",
       "1     1800         1888  \n",
       "2  1551152  2013 (1888)  \n",
       "3   815094         1981  \n",
       "4  1467373         1989  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandp_df.head(5)\n",
    "# so the symbol is the same as the corresponding stock ticker. \n",
    "# It will be used for parsing news results that reference the company that made the headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example code\n",
    "\n",
    "Taken from https://towardsdatascience.com/sentiment-analysis-of-stocks-from-financial-news-using-python-82ebdcefb638"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read news table from finviz (use for process pool executor)\n",
    "def finviz_news_table_process(ticker):\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    \n",
    "    try:\n",
    "        url = finviz_url + ticker\n",
    "        req = Request(url=url, headers={'user-agent': 'my-app/0.0.1'})\n",
    "        response = urlopen(req)\n",
    "        html = BeautifulSoup(response)\n",
    "        news_table = str(html.find(id='news-table'))\n",
    "    except:\n",
    "        news_table = None\n",
    "        \n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    # Return [ticker, str_news_table, run_time, pid]\n",
    "    return [ticker, news_table, end_time - start_time, pid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\envs\\tensor\\lib\\site-packages\\loky\\process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker    pid   run_time                                     str_news_table\n",
      "0    MMM   5264   4.372828  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "1    ABT  10360  11.623676  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "2   ABBV   9880   3.505998  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "3   ABMD  10456   4.262528  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "4    ACN  10244   7.294922  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "5   ATVI   4012   6.728812  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "6   ADBE    216  13.195500  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "7    AMD   2752   9.917668  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "8    AAP   9772   9.825754  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "9    AES   9880   3.493071  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "Process Pool Executor finished in:  273.82866090000005  seconds\n"
     ]
    }
   ],
   "source": [
    "# Process Pool Executor: read html from finviz for each ticker and save the news_table of each as a dataframe\n",
    "\n",
    "from loky import get_reusable_executor\n",
    "import time\n",
    "\n",
    "\n",
    "start_test1 = time.perf_counter()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "    ticker_list = sandp_df['Symbol']\n",
    "    # initiate executor\n",
    "    executor = get_reusable_executor(max_workers=10, timeout=5)\n",
    "    # apply executor to map finviz_news_table on ticker list \n",
    "    process_1 = executor.map(finviz_news_table_process, ticker_list)\n",
    "    # save news tables as a dataframe (includes run time for each request)\n",
    "    news_table_df = pd.DataFrame([[ticker, pid, run_time, str_news_table] for ticker, str_news_table, run_time, pid in process_1], columns=['ticker', 'pid', 'run_time', 'str_news_table'])\n",
    "    print(news_table_df.head(10))\n",
    "    \n",
    "\n",
    "end_test1 = time.perf_counter()\n",
    "\n",
    "print('Process Pool Executor finished in: ', end_test1 - start_test1, ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan for next section\n",
    "\n",
    "Single layer multiprocessing with thread (parallel inside loop outside)  # in progress\n",
    "\n",
    "Single layer multiprocessing with process\n",
    "\n",
    "Single layer multiprocessing with thread (parallel outside loop inside)\n",
    "\n",
    "Single layer multiprocessing with process\n",
    "\n",
    "Double layer multiprocessing with thread (both)\n",
    "\n",
    "Double layer multiprocessing with process\n",
    "\n",
    "Double layer multiprocessing (thread inside, process outside)\n",
    "\n",
    "Double layer multiprocessing (thread outside, process inside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single layer with thread (parallel inside loop outside)\n",
    "Note: In synchronous execution, soupifying the response takes up an overwhelming amount of the total processing time(93%). 2nd highest is waiting for replies at 6%.\n",
    "\n",
    "Based on actual run time, thread appears to significantly save the soupifying time (by 90+%). Theoretically it should save the waiting time as well.\n",
    "\n",
    "Test run time = 191.48199870000008 seconds\n",
    "Estimated full run time = 322.0 minutes 19.68 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to generate [date, time, headline, news_source, content, article_site] \n",
    "# input is a single article\n",
    "def article_details(str_article):\n",
    "    \n",
    "    # convert str to html\n",
    "    html_article = BeautifulSoup(str_article, 'html.parser')\n",
    "    \n",
    "    # Produce headlines\n",
    "    headline = html_article.a.get_text() \n",
    "    \n",
    "    # Produce news source company\n",
    "    news = html_article.span.get_text()\n",
    "    \n",
    "    # Produce Date and Time\n",
    "    # split text in the td tag into a list \n",
    "    date_scrape = html_article.td.text.split()\n",
    "    # ensure most recent date is used\n",
    "    global date\n",
    "    # if the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "    if len(date_scrape) == 1:\n",
    "        time_ = date_scrape[0]\n",
    "    # else load 'date' as the 1st element and 'time' as the second    \n",
    "    else:\n",
    "        date = date_scrape[0]\n",
    "        time_ = date_scrape[1]\n",
    "        \n",
    "    # Produce news content\n",
    "    # get link to the full article\n",
    "    link = html_article.find('a').get('href')\n",
    "    content = 'empty string'\n",
    "    url_root = urlparse(link).netloc\n",
    "    # check if link leads to yahoo.finance\n",
    "    if url_root == 'finance.yahoo.com':\n",
    "        try:\n",
    "            # request from yahoo.finance\n",
    "            req_art = Request(url=link, headers={'user-agent':'my-app/0.0.1'})            \n",
    "            response_art = urlopen(req_art)\n",
    "            html_art = BeautifulSoup(response_art)\n",
    "            # get the article content\n",
    "            content = str(html_art.find(class_='caas-body').get_text())\n",
    "        except:\n",
    "            print('Error following article link: ', link)\n",
    "    \n",
    "    # Return [date, time, headline, news_source, content, article_site] \n",
    "    return [date, time_, headline, news, content, url_root]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record time to parse through each ticker and generate necessary details\n",
    "#time_per_ticker_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to prepare dataframe with [ticker, date, time, headline, news, content, article_site, run_time, req_time, wait_time, soup_time, cont_time]\n",
    "# Thread Pool Processor\n",
    "\n",
    "# input is a row [ticker, pid, run_time, str_news_table] from news_table_df\n",
    "# return completed dataframe for 1 ticker (to be appended)\n",
    "# also records time to process 1 ticker and saves in list 'time_per_ticker'\n",
    "\n",
    "def ticker_to_dataframe_thr(row):\n",
    "    \n",
    "    ticker_time_start = time.perf_counter()\n",
    "    \n",
    "    \n",
    "    # convert str_news_table to html format\n",
    "    html_news_table = BeautifulSoup(row[3], 'html.parser')\n",
    "    # split into list of articles in html format\n",
    "    article_list = html_news_table.findAll('tr')\n",
    "    # convert all html to str\n",
    "    article_list = [str(x) for x in article_list]\n",
    "    \n",
    "    # executor\n",
    "    if __name__ == '__main__':\n",
    "        executor = ThreadPoolExecutor()\n",
    "        thread_2 = executor.map(article_details, article_list)\n",
    "        ticker_df = [[date, time_, news_source, headline, content, site] for date, time_, headline, news_source, content, site in thread_2]\n",
    "        ticker_df = pd.DataFrame(ticker_df, columns=['date', 'time', 'news', 'headline', 'content', 'article_site'])\n",
    "        \n",
    "        \n",
    "    ticker = row[0]\n",
    "    ticker_col = pd.Series([ticker] * len(ticker_df))\n",
    "    \n",
    "    ticker_df.insert(0, 'ticker', ticker_col)\n",
    "        \n",
    "    \n",
    "    ticker_time_end = time.perf_counter()\n",
    "    time_per_ticker_1.append(ticker_time_end - ticker_time_start)\n",
    "\n",
    "    return ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing input\n",
      "  ticker    pid   run_time                                     str_news_table\n",
      "0    MMM   5264   4.372828  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "1    ABT  10360  11.623676  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "2   ABBV   9880   3.505998  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "3   ABMD  10456   4.262528  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "4    ACN  10244   7.294922  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "5   ATVI   4012   6.728812  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "6   ADBE    216  13.195500  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "7    AMD   2752   9.917668  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "8    AAP   9772   9.825754  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "9    AES   9880   3.493071  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "Error following article link:  https://finance.yahoo.com/video/3m-ceo-companys-plans-invest-205807391.html\n",
      "Ticker complete: MMM\n",
      "Error following article link:  https://finance.yahoo.com/news/abbotts-covid-19-antigen-test-185353814.html\n",
      "Ticker complete:  ABT\n",
      "Error following article link:  https://finance.yahoo.com/news/allergan-aesthetics-allergan-abbvie-company-140000028.html\n",
      "Error following article link:  https://finance.yahoo.com/news/recovery-abbvies-abbv-earnings-continue-153703632.html\n",
      "Ticker complete:  ABBV\n",
      "Ticker complete:  ABMD\n",
      "Error following article link:  https://finance.yahoo.com/news/accenture-federal-services-wins-uspto-135900545.html\n",
      "Ticker complete:  ACN\n",
      "Ticker complete:  ATVI\n",
      "Error following article link: Error following article link: Error following article link: Error following article link: Error following article link: Error following article link: Error following article link:  https://finance.yahoo.com/news/why-palantir-long-term-prospects-172551002.html\n",
      "Error following article link:  https://finance.yahoo.com/news/15-biggest-companies-originated-silicon-103049559.html\n",
      "Error following article link: Error following article link:  https://finance.yahoo.com/news/15-best-innovative-stocks-buy-164206577.html\n",
      "Error following article link: Error following article link: Error following article link:       https://finance.yahoo.com/news/former-sac-capital-portfolio-manager-103212152.html https://finance.yahoo.com/news/adobe-systems-adbe-stock-sinks-224510852.html\n",
      "https://finance.yahoo.com/news/nelson-roberts-fund-sees-bright-211447785.html   Error following article link: Error following article link: Error following article link: Error following article link:  \n",
      "Error following article link: https://finance.yahoo.com/news/autodesk-vs-adobe-saas-stock-132744557.html Error following article link:  https://finance.yahoo.com/news/adobe-systems-adbe-gains-market-224510019.htmlError following article link: https://finance.yahoo.com/news/15-best-momentum-stocks-buy-173929374.htmlhttps://finance.yahoo.com/news/buy-adobe-adbe-2021-092025850.html\n",
      "https://finance.yahoo.com/news/adobe-systems-adbe-dips-more-224510591.html\n",
      "https://finance.yahoo.com/news/zacks-analyst-blog-highlights-adobe-152103490.html https://finance.yahoo.com/news/billionaire-ken-fisher-top-10-182120323.html https://finance.yahoo.com/video/young-kenyan-entrepreneur-explains-why-110000096.html\n",
      " https://finance.yahoo.com/news/billionaire-ken-griffin-top-10-160425844.html\n",
      " \n",
      "Error following article link:  https://finance.yahoo.com/news/alteryx-ayx-partners-snowflake-integrate-151303665.html\n",
      "https://finance.yahoo.com/news/heres-why-think-adobe-nasdaq-041728939.html\n",
      "\n",
      "\n",
      "https://finance.yahoo.com/news/adobe-systems-adbe-gains-market-224510390.htmlhttps://finance.yahoo.com/news/top-analyst-reports-adobe-toyota-180006616.htmlhttps://finance.yahoo.com/news/adobe-systems-adbe-outpaces-stock-224510795.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "https://finance.yahoo.com/news/10-best-software-stocks-buy-210429171.html\n",
      "Ticker complete:  ADBE\n",
      "Ticker complete:  AMD\n",
      "Error following article link:  https://finance.yahoo.com/research/reports/ARGUS_27921_MarketSummary_1600041600000\n",
      "Error following article link:  https://finance.yahoo.com/research/reports/ARGUS_2659_AnalystReport_1600041600000\n",
      "Ticker complete:  AAP\n",
      "Error following article link:  https://finance.yahoo.com/news/qatar-wealth-fund-invests-125-113000195.html\n",
      "Error following article link:  https://finance.yahoo.com/news/amlo-nationalist-bent-casts-200-120009291.html\n",
      "Error following article link:  https://finance.yahoo.com/research/reports/ARGUS_27892_MarketUpdate_1599609600000\n",
      "Error following article link:  https://finance.yahoo.com/research/reports/ARGUS_2686_AnalystReport_1599609600000\n",
      "Error following article link:  https://finance.yahoo.com/news/credit-suisse-loan-portfolio-brazil-110000725.html\n",
      "Error following article link:  https://finance.yahoo.com/news/aes-acquires-25-stake-maker-145752664.html\n",
      "Ticker complete:  AES\n",
      "  ticker       date     time                  news  \\\n",
      "0    MMM  Jan-25-21  09:37AM              Benzinga   \n",
      "1    MMM  Jan-26-21  04:18PM         Yahoo Finance   \n",
      "2    MMM  Jan-26-21  03:58PM   Yahoo Finance Video   \n",
      "3    MMM  Jan-13-21  08:00AM           PR Newswire   \n",
      "4    MMM  Feb-14-21  09:27AM           Motley Fool   \n",
      "5    MMM  Dec-15-20  05:12PM         GuruFocus.com   \n",
      "6    MMM  Feb-12-21  10:10AM           Motley Fool   \n",
      "7    MMM  Feb-12-21  07:45AM           Motley Fool   \n",
      "8    MMM  Feb-10-21  06:00AM           Barrons.com   \n",
      "9    MMM  Jan-26-21  02:35PM           PR Newswire   \n",
      "\n",
      "                                            headline  \\\n",
      "0                               ROCE Insights For 3M   \n",
      "1  Why 3M is spending $1 billion to help improve ...   \n",
      "2  3M CEO on the company's plans to invest $1B to...   \n",
      "3  3M to Invest $1 Billion to Achieve Carbon Neut...   \n",
      "4    These 3 Dividend Stocks Are Too Cheap to Ignore   \n",
      "5    First Eagle Investment's Top 4th-Quarter Trades   \n",
      "6                  Why 3M Is a Retiree's Dream Stock   \n",
      "7                3 Top Value Stocks to Buy Right Now   \n",
      "8  How a $1.9B Bond Fund Finds Opportunity in Mar...   \n",
      "9              3M Announces Upcoming Investor Events   \n",
      "\n",
      "                                             content       article_site   pid  \\\n",
      "0  3M (NYSE:MMM) posted a 3.14% decrease in earni...  finance.yahoo.com  7176   \n",
      "1  3M Chairman and CEO Mike Roman said it's time ...  finance.yahoo.com  7176   \n",
      "2                                       empty string  finance.yahoo.com  7176   \n",
      "3  ST. PAUL, Minn., Feb. 16, 2021 /PRNewswire/ --...  finance.yahoo.com  7176   \n",
      "4                                       empty string       www.fool.com  7176   \n",
      "5  - By Graham GriffinFirst Eagle Investment (Tra...  finance.yahoo.com  7176   \n",
      "6                                       empty string       www.fool.com  7176   \n",
      "7                                       empty string       www.fool.com  7176   \n",
      "8                                       empty string    www.barrons.com  7176   \n",
      "9  ST. PAUL, Minn., Feb. 9, 2021 /PRNewswire/ -- ...  finance.yahoo.com  7176   \n",
      "\n",
      "    run_time  req_time  wait_time  soup_time    cont_time  \n",
      "0  26.672848  0.000031   0.770812  25.893972     0.007364  \n",
      "1   7.715878  0.000042   0.544151   7.154470     0.015924  \n",
      "2  10.958002  0.000034   4.743762   6.211582 -2289.370570  \n",
      "3  27.221123  0.000043   0.899912  26.312425     0.008124  \n",
      "4   0.000565  0.000000   0.000000   0.000000     0.000000  \n",
      "5  28.089358  0.000028   0.516932  27.561398     0.010284  \n",
      "6   0.000775  0.000000   0.000000   0.000000     0.000000  \n",
      "7   0.000550  0.000000   0.000000   0.000000     0.000000  \n",
      "8   0.000598  0.000000   0.000000   0.000000     0.000000  \n",
      "9  26.398833  0.000028   0.828473  25.562487     0.007232  \n",
      "Time taken to compile 5 tickers is:  847.0188143  seconds\n",
      "Estimated time to compile all tickers is:  712.0 minutes 54.45012214999588 seconds\n"
     ]
    }
   ],
   "source": [
    "# test run thread\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# define test df\n",
    "test_news_table_df = news_table_df.copy()\n",
    "test_news_table_df = test_news_table_df.iloc[0:10]\n",
    "print('Testing input')\n",
    "print(test_news_table_df)\n",
    "\n",
    "time_per_ticker_1 = []\n",
    "\n",
    "compile_start = time.perf_counter()\n",
    "\n",
    "test_art_det_df = ticker_to_dataframe_thr(test_news_table_df.iloc[0])\n",
    "print('Ticker complete: MMM')\n",
    "\n",
    "\n",
    "for i in range(1, len(test_news_table_df)):\n",
    "    test_ticker_df = ticker_to_dataframe_thr(test_news_table_df.iloc[i])\n",
    "    test_art_det_df = test_art_det_df.append(test_ticker_df)\n",
    "    print('Ticker complete: ', test_ticker_df.iloc[0,0])\n",
    "\n",
    "compile_end = time.perf_counter()\n",
    "    \n",
    "print(test_art_det_df.head(10))\n",
    "print('Time taken to compile 5 tickers is: ', compile_end - compile_start, ' seconds')\n",
    "minutes = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) // 60 \n",
    "seconds = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) % 60\n",
    "print('Estimated time to compile all tickers is: ', minutes, 'minutes', seconds, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74.3868666000003,\n",
       " 141.48550909999994,\n",
       " 115.20991210000011,\n",
       " 147.0775291,\n",
       " 101.96044429999984,\n",
       " 96.89249870000003,\n",
       " 52.285079400000086,\n",
       " 41.166900599999735,\n",
       " 36.71929769999997,\n",
       " 39.784834200000205]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time per ticker\n",
    "time_per_ticker_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single layer with process (parallel inside loop outside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to generate [date, time, headline, news_source, content, article_site, pid] \n",
    "# input is a single article\n",
    "# adjusted for process pool executor\n",
    "\n",
    "def article_details_v2(date_time, str_article):\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    \n",
    "    # Produce date and time\n",
    "    date = date_time[0]\n",
    "    time_ = date_time[1]\n",
    "    \n",
    "    # convert str to html\n",
    "    html_article = BeautifulSoup(str_article, 'html.parser')\n",
    "    \n",
    "    # Produce headlines\n",
    "    headline = html_article.a.get_text() \n",
    "    \n",
    "    # Produce news source company\n",
    "    news = html_article.span.get_text()\n",
    "    \n",
    "    # Produce news content\n",
    "    # get link to the full article\n",
    "    link = html_article.find('a').get('href')\n",
    "    content = 'empty string'\n",
    "    url_root = urlparse(link).netloc\n",
    "    # check if link leads to yahoo.finance\n",
    "    if url_root == 'finance.yahoo.com':\n",
    "        try:\n",
    "            # request from yahoo.finance\n",
    "            req_art = Request(url=link, headers={'user-agent':'my-app/0.0.1'})\n",
    "            response_art = urlopen(req_art)\n",
    "            html_art = BeautifulSoup(response_art)\n",
    "            # get the article content\n",
    "            content = str(html_art.find(class_='caas-body').get_text())\n",
    "        except:\n",
    "            print('Error following article link: ', link)\n",
    "    \n",
    "    \n",
    "    # Return [date, time, headline, news_source, content, article_site, pid,] \n",
    "    return [date, time_, headline, news, content, url_root, pid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_list(html_article_list):\n",
    "    date_time = []\n",
    "    for html_article in html_article_list:\n",
    "        date_scrape = html_article.td.text.split()\n",
    "        if len(date_scrape) == 1:\n",
    "            time_ = date_scrape[0]\n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time_ = date_scrape[1]\n",
    "        date_time.append([date, time_])\n",
    "        \n",
    "        \n",
    "    return date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to prepare data with ticker, date, time, headline, news, content\n",
    "# Process Pool Processor\n",
    "\n",
    "# input is a row [ticker, pid, run_time, str_news_table]\n",
    "# return completed dataframe for 1 ticker (to be appended)\n",
    "# also records time to process 1 ticker and saves in list 'time_per_ticker'\n",
    "\n",
    "def ticker_to_dataframe_pro(row):\n",
    "    \n",
    "    ticker_time_start = time.perf_counter()\n",
    "    \n",
    "    # convert str_news_table to html format\n",
    "    html_news_table = BeautifulSoup(row[3], 'html.parser')\n",
    "    # split into list of articles in html format\n",
    "    article_list = html_news_table.findAll('tr')\n",
    "    # get date and time\n",
    "    date_time_list = date_to_list(article_list)\n",
    "    # convert all html to str\n",
    "    article_list = [str(x) for x in article_list]\n",
    "    \n",
    "    # section of code to be looped\n",
    "    if __name__ == '__main__':\n",
    "        executor = get_reusable_executor(max_workers=5, timeout=5)\n",
    "        process_2 = executor.map(article_details_v2, date_time_list, article_list)\n",
    "        ticker_df = [[date, time_, news_source, headline, content, site, pid] for date, time_, headline, news_source, content, site, pid in process_2]\n",
    "        ticker_df = pd.DataFrame(ticker_df, columns=['date', 'time', 'news', 'headline', 'content', 'article_site', 'pid'])\n",
    "        \n",
    "    ticker = row[0]\n",
    "    ticker_col = pd.Series([ticker] * len(ticker_df))\n",
    "    \n",
    "    \n",
    "    ticker_df.insert(0, 'ticker', ticker_col)\n",
    "        \n",
    "    \n",
    "    ticker_time_end = time.perf_counter()\n",
    "    time_per_ticker_1.append(ticker_time_end - ticker_time_start)\n",
    "\n",
    "    return ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing input\n",
      "  ticker    pid   run_time                                     str_news_table\n",
      "0    MMM   5264   4.372828  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "1    ABT  10360  11.623676  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "2   ABBV   9880   3.505998  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "3   ABMD  10456   4.262528  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "4    ACN  10244   7.294922  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "5   ATVI   4012   6.728812  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "6   ADBE    216  13.195500  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "7    AMD   2752   9.917668  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "8    AAP   9772   9.825754  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "9    AES   9880   3.493071  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "Ticker complete: MMM\n",
      "Ticker complete:  ABT\n",
      "Ticker complete:  ABBV\n",
      "Ticker complete:  ABMD\n",
      "Ticker complete:  ACN\n",
      "Ticker complete:  ATVI\n",
      "Ticker complete:  ADBE\n",
      "Ticker complete:  AMD\n",
      "Ticker complete:  AAP\n",
      "Ticker complete:  AES\n",
      "  ticker       date     time                  news  \\\n",
      "0    MMM  Feb-17-21  09:37AM              Benzinga   \n",
      "1    MMM  Feb-16-21  04:18PM         Yahoo Finance   \n",
      "2    MMM  Feb-16-21  03:58PM   Yahoo Finance Video   \n",
      "3    MMM  Feb-16-21  08:00AM           PR Newswire   \n",
      "4    MMM  Feb-14-21  09:27AM           Motley Fool   \n",
      "5    MMM  Feb-12-21  05:12PM         GuruFocus.com   \n",
      "6    MMM  Feb-12-21  10:10AM           Motley Fool   \n",
      "7    MMM  Feb-12-21  07:45AM           Motley Fool   \n",
      "8    MMM  Feb-10-21  06:00AM           Barrons.com   \n",
      "9    MMM  Feb-09-21  02:35PM           PR Newswire   \n",
      "\n",
      "                                            headline  \\\n",
      "0                               ROCE Insights For 3M   \n",
      "1  Why 3M is spending $1 billion to help improve ...   \n",
      "2  3M CEO on the company's plans to invest $1B to...   \n",
      "3  3M to Invest $1 Billion to Achieve Carbon Neut...   \n",
      "4    These 3 Dividend Stocks Are Too Cheap to Ignore   \n",
      "5    First Eagle Investment's Top 4th-Quarter Trades   \n",
      "6                  Why 3M Is a Retiree's Dream Stock   \n",
      "7                3 Top Value Stocks to Buy Right Now   \n",
      "8  How a $1.9B Bond Fund Finds Opportunity in Mar...   \n",
      "9              3M Announces Upcoming Investor Events   \n",
      "\n",
      "                                             content       article_site    pid  \n",
      "0  3M (NYSE:MMM) posted a 3.14% decrease in earni...  finance.yahoo.com  15056  \n",
      "1  3M Chairman and CEO Mike Roman said it's time ...  finance.yahoo.com  15432  \n",
      "2  Mike Roman, 3M Chairman and CEO, joined Yahoo ...  finance.yahoo.com   8232  \n",
      "3  ST. PAUL, Minn., Feb. 16, 2021 /PRNewswire/ --...  finance.yahoo.com  12772  \n",
      "4                                       empty string       www.fool.com   7784  \n",
      "5  - By Graham GriffinFirst Eagle Investment (Tra...  finance.yahoo.com   7784  \n",
      "6                                       empty string       www.fool.com   8232  \n",
      "7                                       empty string       www.fool.com   8232  \n",
      "8                                       empty string    www.barrons.com   8232  \n",
      "9  ST. PAUL, Minn., Feb. 9, 2021 /PRNewswire/ -- ...  finance.yahoo.com   8232  \n",
      "Time taken to compile 10 tickers is:  479.7864262000003  seconds\n",
      "Estimated time to compile all tickers is:  403.0 minutes 49.21452310001405 seconds\n"
     ]
    }
   ],
   "source": [
    "# test run process (max_workers = 5)\n",
    "\n",
    "import time\n",
    "\n",
    "# define test df\n",
    "test_news_table_df = news_table_df.copy()\n",
    "test_news_table_df = test_news_table_df.iloc[0:10]\n",
    "print('Testing input')\n",
    "print(test_news_table_df)\n",
    "\n",
    "time_per_ticker_1 = []\n",
    "\n",
    "compile_start = time.perf_counter()\n",
    "\n",
    "test_art_det_df = ticker_to_dataframe_pro(test_news_table_df.iloc[0])\n",
    "print('Ticker complete: MMM')\n",
    "\n",
    "\n",
    "for i in range(1, len(test_news_table_df)):\n",
    "    test_ticker_df = ticker_to_dataframe_pro(test_news_table_df.iloc[i])\n",
    "    test_art_det_df = test_art_det_df.append(test_ticker_df)\n",
    "    print('Ticker complete: ', test_ticker_df.iloc[0,0])\n",
    "\n",
    "compile_end = time.perf_counter()\n",
    "    \n",
    "print(test_art_det_df.head(10))\n",
    "print('Time taken to compile 10 tickers is: ', compile_end - compile_start, ' seconds')\n",
    "minutes = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) // 60 \n",
    "seconds = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) % 60\n",
    "print('Estimated time to compile all tickers is: ', minutes, 'minutes', seconds, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to prepare data with ticker, date, time, headline, news, content\n",
    "# Process Pool Processor\n",
    "\n",
    "# input is a row [ticker, pid, run_time, str_news_table]\n",
    "# return completed dataframe for 1 ticker (to be appended)\n",
    "# also records time to process 1 ticker and saves in list 'time_per_ticker'\n",
    "\n",
    "def ticker_to_dataframe_prov2(row):\n",
    "    \n",
    "    ticker_time_start = time.perf_counter()\n",
    "    \n",
    "    # convert str_news_table to html format\n",
    "    html_news_table = BeautifulSoup(row[3], 'html.parser')\n",
    "    # split into list of articles in html format\n",
    "    article_list = html_news_table.findAll('tr')\n",
    "    # get date and time\n",
    "    date_time_list = date_to_list(article_list)\n",
    "    # convert all html to str\n",
    "    article_list = [str(x) for x in article_list]\n",
    "    \n",
    "    # section of code to be looped\n",
    "    if __name__ == '__main__':\n",
    "        executor = get_reusable_executor(max_workers=10, timeout=5)\n",
    "        process_2 = executor.map(article_details_v2, date_time_list, article_list)\n",
    "        ticker_df = [[date, time_, news_source, headline, content, site, pid] for date, time_, headline, news_source, content, site, pid in process_2]\n",
    "        ticker_df = pd.DataFrame(ticker_df, columns=['date', 'time', 'news', 'headline', 'content', 'article_site', 'pid'])\n",
    "        \n",
    "    ticker = row[0]\n",
    "    ticker_col = pd.Series([ticker] * len(ticker_df))\n",
    "    \n",
    "    \n",
    "    ticker_df.insert(0, 'ticker', ticker_col)\n",
    "        \n",
    "    \n",
    "    ticker_time_end = time.perf_counter()\n",
    "    time_per_ticker_1.append(ticker_time_end - ticker_time_start)\n",
    "\n",
    "    return ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing input\n",
      "  ticker    pid   run_time                                     str_news_table\n",
      "0    MMM   5264   4.372828  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "1    ABT  10360  11.623676  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "2   ABBV   9880   3.505998  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "3   ABMD  10456   4.262528  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "4    ACN  10244   7.294922  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "5   ATVI   4012   6.728812  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "6   ADBE    216  13.195500  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "7    AMD   2752   9.917668  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "8    AAP   9772   9.825754  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "9    AES   9880   3.493071  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "Ticker complete: MMM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\envs\\tensor\\lib\\site-packages\\loky\\process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker complete:  ABT\n",
      "Ticker complete:  ABBV\n",
      "Ticker complete:  ABMD\n",
      "Ticker complete:  ACN\n",
      "Ticker complete:  ATVI\n",
      "Ticker complete:  ADBE\n",
      "Ticker complete:  AMD\n",
      "Ticker complete:  AAP\n",
      "Ticker complete:  AES\n",
      "  ticker       date     time                  news  \\\n",
      "0    MMM  Feb-17-21  09:37AM              Benzinga   \n",
      "1    MMM  Feb-16-21  04:18PM         Yahoo Finance   \n",
      "2    MMM  Feb-16-21  03:58PM   Yahoo Finance Video   \n",
      "3    MMM  Feb-16-21  08:00AM           PR Newswire   \n",
      "4    MMM  Feb-14-21  09:27AM           Motley Fool   \n",
      "5    MMM  Feb-12-21  05:12PM         GuruFocus.com   \n",
      "6    MMM  Feb-12-21  10:10AM           Motley Fool   \n",
      "7    MMM  Feb-12-21  07:45AM           Motley Fool   \n",
      "8    MMM  Feb-10-21  06:00AM           Barrons.com   \n",
      "9    MMM  Feb-09-21  02:35PM           PR Newswire   \n",
      "\n",
      "                                            headline  \\\n",
      "0                               ROCE Insights For 3M   \n",
      "1  Why 3M is spending $1 billion to help improve ...   \n",
      "2  3M CEO on the company's plans to invest $1B to...   \n",
      "3  3M to Invest $1 Billion to Achieve Carbon Neut...   \n",
      "4    These 3 Dividend Stocks Are Too Cheap to Ignore   \n",
      "5    First Eagle Investment's Top 4th-Quarter Trades   \n",
      "6                  Why 3M Is a Retiree's Dream Stock   \n",
      "7                3 Top Value Stocks to Buy Right Now   \n",
      "8  How a $1.9B Bond Fund Finds Opportunity in Mar...   \n",
      "9              3M Announces Upcoming Investor Events   \n",
      "\n",
      "                                             content       article_site    pid  \n",
      "0  3M (NYSE:MMM) posted a 3.14% decrease in earni...  finance.yahoo.com  15668  \n",
      "1  3M Chairman and CEO Mike Roman said it's time ...  finance.yahoo.com  12456  \n",
      "2  Mike Roman, 3M Chairman and CEO, joined Yahoo ...  finance.yahoo.com  10564  \n",
      "3  ST. PAUL, Minn., Feb. 16, 2021 /PRNewswire/ --...  finance.yahoo.com   6660  \n",
      "4                                       empty string       www.fool.com  15776  \n",
      "5  - By Graham GriffinFirst Eagle Investment (Tra...  finance.yahoo.com  15776  \n",
      "6                                       empty string       www.fool.com  15668  \n",
      "7                                       empty string       www.fool.com  15668  \n",
      "8                                       empty string    www.barrons.com  15668  \n",
      "9  ST. PAUL, Minn., Feb. 9, 2021 /PRNewswire/ -- ...  finance.yahoo.com  15668  \n",
      "Time taken to compile 10 tickers is:  467.38618250000036  seconds\n",
      "Estimated time to compile all tickers is:  393.0 minutes 23.002216250017227 seconds\n"
     ]
    }
   ],
   "source": [
    "# test run process (max_workers = 10)\n",
    "\n",
    "import time\n",
    "\n",
    "# define test df\n",
    "test_news_table_df = news_table_df.copy()\n",
    "test_news_table_df = test_news_table_df.iloc[0:10]\n",
    "print('Testing input')\n",
    "print(test_news_table_df)\n",
    "\n",
    "time_per_ticker_1 = []\n",
    "\n",
    "compile_start = time.perf_counter()\n",
    "\n",
    "test_art_det_df = ticker_to_dataframe_pro(test_news_table_df.iloc[0])\n",
    "print('Ticker complete: MMM')\n",
    "\n",
    "\n",
    "for i in range(1, len(test_news_table_df)):\n",
    "    test_ticker_df = ticker_to_dataframe_prov2(test_news_table_df.iloc[i])\n",
    "    test_art_det_df = test_art_det_df.append(test_ticker_df)\n",
    "    print('Ticker complete: ', test_ticker_df.iloc[0,0])\n",
    "\n",
    "compile_end = time.perf_counter()\n",
    "    \n",
    "print(test_art_det_df.head(10))\n",
    "print('Time taken to compile 10 tickers is: ', compile_end - compile_start, ' seconds')\n",
    "minutes = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) // 60 \n",
    "seconds = ((compile_end - compile_start) * len(news_table_df)/len(test_news_table_df) ) % 60\n",
    "print('Estimated time to compile all tickers is: ', minutes, 'minutes', seconds, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Pool Executor\n",
    "\n",
    "from loky import get_reusable_executor\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "start_test1 = time.perf_counter()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_news_tables = news_tables.copy()\n",
    "    test_news_tables = dict(zip(['MMM', 'ABT', 'ABBV'], [str(test_news_tables[x]) for x in ['MMM', 'ABT', 'ABBV']]))\n",
    "    executor = get_reusable_executor(max_workers=4, timeout=2)\n",
    "    p = executor.submit(prepare_data, test_news_tables)\n",
    "    print('Processor ID: ', p.result())\n",
    "        \n",
    "        \n",
    "columns = ['ticker', 'date', 'time', 'headline', 'news', 'content']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_news_updated'\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "ori_len = len(parsed_news_updated)\n",
    "# remove articles whose contents are not available\n",
    "parsed_news_updated_cont = parsed_news_updated.copy()\n",
    "parsed_news_updated_cont = parsed_news_updated_cont.loc[parsed_news_updated_cont['content'] != 'empty string']\n",
    "\n",
    "parsed_news_updated = parsed_news_updated.drop('content', axis=1)\n",
    "\n",
    "print('Percentage decrease in articles is: ', (ori_len - len(parsed_news_updated_cont))/ori_len)\n",
    "\n",
    "\n",
    "print(parsed_news_updated_cont.head(10))\n",
    "print(rejected_sites[:20])\n",
    "\n",
    "end_test1 = time.perf_counter()\n",
    "\n",
    "print('Process Pool Executor finished in: ', end_test1 - start_test1, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread Pool Executor\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "start_test1 = time.perf_counter()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_news_tables = news_tables.copy()\n",
    "    test_news_tables = dict(zip(['MMM', 'ABT', 'ABBV'], [str(test_news_tables[x]) for x in ['MMM', 'ABT', 'ABBV']]))\n",
    "    executor = ThreadPoolExecutor()\n",
    "    p = executor.submit(prepare_data, test_news_tables)\n",
    "    print(p.result())\n",
    "        \n",
    "        \n",
    "columns = ['ticker', 'date', 'time', 'headline', 'news', 'content']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_news_updated'\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "ori_len = len(parsed_news_updated)\n",
    "# remove articles whose contents are not available\n",
    "parsed_news_updated_cont = parsed_news_updated.copy()\n",
    "parsed_news_updated_cont = parsed_news_updated_cont.loc[parsed_news_updated_cont['content'] != 'empty string']\n",
    "\n",
    "parsed_news_updated = parsed_news_updated.drop('content', axis=1)\n",
    "\n",
    "print('Percentage decrease in articles is: ', (ori_len - len(parsed_news_updated_cont))/ori_len)\n",
    "\n",
    "\n",
    "print(parsed_news_updated_cont.head(10))\n",
    "print(rejected_sites[:20])\n",
    "\n",
    "end_test1 = time.perf_counter()\n",
    "\n",
    "print('Thread Pool Executor finished in: ', end_test1 - start_test1, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "start_test2 = time.perf_counter()\n",
    "\n",
    "\n",
    "test_news_tables = news_tables.copy()\n",
    "test_news_tables = dict(zip(['MMM', 'ABT', 'ABBV'], [str(test_news_tables[x]) for x in ['MMM', 'ABT', 'ABBV']]))\n",
    "print(prepare_data(test_news_tables))\n",
    "\n",
    "# Set column names\n",
    "columns = ['ticker', 'date', 'time', 'headline', 'news', 'content']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "ori_len = len(parsed_news_updated)\n",
    "# remove articles whose contents are not available\n",
    "parsed_news_updated_content = parsed_news_updated.copy()\n",
    "parsed_news_updated_content = parsed_news_updated_content.loc[parsed_news_updated['content'] != 'empty string']\n",
    "parsed_news_updated = parsed_news_updated.drop('content', axis=1)\n",
    "print('Percentage decrease in articles is: ', (ori_len - len(parsed_news_updated_content))/ori_len)\n",
    "\n",
    "\n",
    "print(parsed_news_updated_content.head(10))\n",
    "print(rejected_sites[:20])\n",
    "\n",
    "\n",
    "end_test2 = time.perf_counter()\n",
    "\n",
    "print('Synchronous test finished in: ', end_test2 - start_test2, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:34:38.201871Z",
     "start_time": "2021-02-13T02:34:34.857909Z"
    }
   },
   "outputs": [],
   "source": [
    "parsed_news = []\n",
    "rejected_sites = []\n",
    "\n",
    "# Iterate through the news\n",
    "for file_name, news_table in news_tables.items():\n",
    "    # Iterate through all tr tags in 'news_table'\n",
    "    for x in news_table.findAll('tr'):\n",
    "        art_content = article_content(x)\n",
    "        # combine article summary and article content into 1 row of info about the article\n",
    "        article_row = article_summary(x) + [art_content[0]]\n",
    "        # Append relevant info as a list to the 'parsed_news' list\n",
    "        parsed_news.append(article_row)\n",
    "        rejected_sites.append(art_content[1])\n",
    "\n",
    "# Set column names\n",
    "columns = ['ticker', 'date', 'time', 'headline', 'news', 'content']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "ori_len = len(parsed_news_updated)\n",
    "# remove articles whose contents are not available\n",
    "parsed_news_updated_content = parsed_news_updated.copy()\n",
    "parsed_news_updated_content = parsed_news_updated_content.loc[parsed_news_updated['content'] != 'empty string']\n",
    "parsed_news_updated = parsed_news_updated.drop('content', axis=1)\n",
    "print('Percentage decrease in articles is: ', (ori_len - len(parsed_news_updated_content))/ori_len)\n",
    "parsed_news_updated_content.head(10)\n",
    "\n",
    "#parsed_news_updated does not have the content column and has full number of articles (100 per ticker)\n",
    "#parsed_news_updated_content has content column and is shortened to remove all rows with no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of headlines produced by each news source and remove news sources with <median headlines\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "# generate series of counts\n",
    "news_count = parsed_news_updated['news'].value_counts()\n",
    "# set minimum count = median\n",
    "cutoff_point = news_count.median()\n",
    "# append count of news to dataframe\n",
    "parsed_news_updated = parsed_news_updated.merge(news_count, left_on='news', right_index=True)\n",
    "parsed_news_updated = parsed_news_updated.drop('news_x', axis=1).rename({'news_y' : 'count'}, axis=1)\n",
    "# remove news which have count < cutoff_point\n",
    "parsed_news_updated = parsed_news_updated.loc[parsed_news_updated['count'] > cutoff_point]\n",
    "parsed_news_updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:34:48.450433Z",
     "start_time": "2021-02-13T02:34:41.894994Z"
    }
   },
   "outputs": [],
   "source": [
    "#need to tokenize each words within the headlines to improve the sentiment score.\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def regex(x):\n",
    "    special_chars_p = \"[.'&$\\\"\\-()#@!?/:]\"\n",
    "    s1 = re.sub(special_chars_p, '', x)  \n",
    "    return(s1)\n",
    "\n",
    "parsed_news_updated['headline'] = parsed_news_updated['headline'].apply(regex)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.lower().split()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "parsed_news_updated_stem = parsed_news_updated.copy()\n",
    "parsed_news_updated_stem['headline'] = parsed_news_updated_stem['headline'].apply(stem_sentences)\n",
    "\n",
    "stop=stopwords.words('english')\n",
    "\n",
    "parsed_news_updated['headline'].apply(lambda x: [item for item in x if item not in stop])\n",
    "parsed_news_updated_stem['headline'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "parsed_news_updated['headline'] = parsed_news_updated['headline'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) \n",
    "parsed_news_updated_stem['headline'] = parsed_news_updated_stem['headline'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) \n",
    "parsed_news_updated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:34:56.695664Z",
     "start_time": "2021-02-13T02:34:50.966410Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NLTK VADER for sentiment analysis (unstem)\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Instantiate the sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Iterate through the headlines and get the polarity scores using vader\n",
    "scores = parsed_news_updated['headline'].apply(vader.polarity_scores).tolist()\n",
    "\n",
    "# Convert the 'scores' list of dicts into a DataFrame\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# Join the DataFrames of the news and the list of dicts\n",
    "parsed_and_scored_news = parsed_news_updated.join(scores_df, rsuffix='_right')\n",
    "\n",
    "# Convert the date column from string to datetime\n",
    "parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.date\n",
    "\n",
    "parsed_and_scored_news.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK VADER for sentiment analysis (stem)\n",
    "\n",
    "# Instantiate the sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Iterate through the headlines and get the polarity scores using vader\n",
    "scores_stem = parsed_news_updated_stem['headline'].apply(vader.polarity_scores).tolist()\n",
    "\n",
    "# Convert the 'scores' list of dicts into a DataFrame\n",
    "scores_stem_df = pd.DataFrame(scores_stem)\n",
    "\n",
    "# Join the DataFrames of the news and the list of dicts\n",
    "parsed_and_scored_news_stem = parsed_news_updated_stem.join(scores_stem_df, rsuffix='_right')\n",
    "\n",
    "# Convert the date column from string to datetime\n",
    "parsed_and_scored_news_stem['date'] = pd.to_datetime(parsed_and_scored_news_stem.date).dt.date\n",
    "\n",
    "parsed_and_scored_news_stem.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:35:12.704918Z",
     "start_time": "2021-02-13T02:35:12.633876Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get S&P 500 prices\n",
    "# source: https://www.spglobal.com/spdji/en/indices/equity/sp-500/#overview\n",
    "\n",
    "df_sp = pd.read_csv('data/S&P500_5years.csv', usecols=[0,1]) # Use only first 2 columns\n",
    "df_sp.columns = ['date', 'price']\n",
    "df_sp['date'] = pd.to_datetime(df_sp['date'])\n",
    "df_sp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:36:17.050503Z",
     "start_time": "2021-02-13T02:35:14.178417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get S&P 500 individual stock prices\n",
    "\n",
    "# Create a function to get stock price given a ticker \n",
    "def get_stock_price(ticker, start, end):\n",
    "    '''Get prices of a stock in a given period.\n",
    "    \n",
    "    Args:\n",
    "        ticker (str): ticker of a company \n",
    "        start (str): date in format of 'YYYY-MM-DD'\n",
    "        end (str): date in format of 'YYYY-MM-DD'\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing open, high, low, close, volume, dividends, stock splits\n",
    "    '''\n",
    "    import yfinance as yf\n",
    "    \n",
    "    ticker = yf.Ticker(ticker)\n",
    "    data = ticker.history(start=start, end=end)\n",
    "    data.reset_index(level=0, inplace=True)\n",
    "    return data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate merged dataframe (merged by date)\n",
    "# columns: ['ticker', 'date', 'time', 'headline', 'news', 'neg', 'neu', 'pos', 'compound', 'open', 'close', 'change']\n",
    "# scored dataframe should be the input (not sure if works for scored sentiment other than Vader)\n",
    "\n",
    "\n",
    "def generate_final_df(scored_df):\n",
    "    # Get a list of 505 stocks from S&P 500\n",
    "    sp500 = sandp_df['Symbol'].unique()\n",
    "    start = scored_df['date'].min()\n",
    "    end = scored_df['date'].max()\n",
    "    \n",
    "    # Iterate through each stock to get price\n",
    "    df_stock = pd.DataFrame()\n",
    "    for ticker in sp500:\n",
    "        data = get_stock_price(ticker, start, end)\n",
    "        data['ticker'] = ticker\n",
    "        df_stock = pd.concat([df_stock, data], axis=0)\n",
    "        \n",
    "    # Change all columns names to lowercase  \n",
    "    df_stock.columns = df_stock.columns.str.lower()\n",
    "    \n",
    "    # Convert timestamp to date\n",
    "    df_stock['date'] = df_stock['date'].apply(datetime.date)\n",
    "    \n",
    "    # Reset index\n",
    "    df_stock.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Merge stock price info and sentiment scores\n",
    "    df_merged = scored_df.merge(df_stock.loc[:, ['date', 'ticker', 'open', 'close']], on=['date', 'ticker'])\n",
    "    # Add column: price change\n",
    "    df_merged['change'] = df_merged['close'] - df_merged['open']\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final df for unstem and stem\n",
    "\n",
    "df_final_unstem = generate_final_df(parsed_and_scored_news)\n",
    "df_final_stem = generate_final_df(parsed_and_scored_news_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:37:43.967232Z",
     "start_time": "2021-02-13T02:37:43.917201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate pearson correlation coef between sentiment score and price for each news media\n",
    "scores_close_unstem = df_final_unstem.groupby('news')[['compound', 'close']].corr().unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_close_stem = df_final_stem.groupby('news')[['compound', 'close']].corr().unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_change_unstem = df_final_unstem.groupby('news')[['compound', 'change']].corr().unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_change_stem = df_final_stem.groupby('news')[['compound', 'change']].corr().unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/28988627/pandas-correlation-groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate spearman correlation coef between sentiment score and price for each news media\n",
    "scores_close_unstem = df_final_unstem.groupby('news')[['compound', 'close']].corr(method='spearman').unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_close_stem = df_final_stem.groupby('news')[['compound', 'close']].corr(method='spearman').unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_change_unstem = df_final_unstem.groupby('news')[['compound', 'change']].corr(method='spearman').unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_change_stem = df_final_stem.groupby('news')[['compound', 'change']].corr(method='spearman').unstack().iloc[:, 1].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson's Correlation Coefficient as Dataframe\n",
    "pearson_corr = pd.DataFrame({'variable' : ['close', 'change'], 'unstem' : [df_final_unstem[['compound', 'close']].corr().iloc[0,1], df_final_unstem[['compound', 'change']].corr().iloc[0,1]], 'stem' : [df_final_stem[['compound', 'close']].corr().iloc[0,1], df_final_stem[['compound', 'change']].corr().iloc[0,1]]})\n",
    "pearson_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman's rank correlation\n",
    "spearman_corr = pd.DataFrame({'variable' : ['close', 'change'], 'unstem' : [df_final_unstem[['compound', 'close']].corr(method='spearman').iloc[0,1], df_final_unstem[['compound', 'change']].corr(method='spearman').iloc[0,1]], 'stem' : [df_final_stem[['compound', 'close']].corr(method='spearman').iloc[0,1], df_final_stem[['compound', 'change']].corr(method='spearman').iloc[0,1]]})\n",
    "spearman_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vader Sentiment Analysis\n",
    "\n",
    "Performed Pearson's Correlation Coefficient comparisons between compound sentiment and (1) closing price (2) change in price (closing price - opening price). Computed the Spearman rank correlation coefficient as well.\n",
    "\n",
    "Compared effect of stemming and not stemming words on Pearson's and Spearman's correlation coefficient.\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Pearson:\n",
    "Correlation between both (1) and (2) is negligible (<1%) without stemming. Stemming appears to improve correlation, but correlation is still very small (<2%)\n",
    "\n",
    "Spearman:\n",
    "Correlation for both (1) and (2) is still small, but better than Pearson. Stemming has inconsistent results, slightly lowering (2) but increasing (1)\n",
    "\n",
    "Overall, Vader sentiment analysis produces very weak correlation with both (1) and (2). Try with other models.\n",
    "\n",
    "Removing headlines from news sources with less articles:\n",
    "Decreased correlation. Most of the correlations are negative but close to 0. This suggests that news sources with lower article counts tend to predict the price movements more accurately than the news sources that post more often.\n",
    "\n",
    "\n",
    "Note: \n",
    "\n",
    "1. Might be helpful to determine the most relevant news sources by taking highly correlated news sources with instances of more than 20. Limiting the data might increase correlation. Use test set to evaluate if using this method.\n",
    "\n",
    "2. Doing linear regression on neg, neu and pos score might produce interesting results.\n",
    "\n",
    "3. Vader sentiment scores might be the problem. Sentiment scores of some sample headlines was observed and Vader had many false negatives. Try using article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:38:23.792228Z",
     "start_time": "2021-02-13T02:38:23.778225Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final_unstem.loc[df_final_unstem['news']==' The Telegraph', ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. Take note of changes in the composition of S&P 500."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
