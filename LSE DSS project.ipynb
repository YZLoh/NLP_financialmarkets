{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Collect-S&amp;P-500-Companies\" data-toc-modified-id=\"Collect-S&amp;P-500-Companies-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Collect S&amp;P 500 Companies</a></span></li><li><span><a href=\"#Example-code\" data-toc-modified-id=\"Example-code-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Example code</a></span></li><li><span><a href=\"#Stock-Prices\" data-toc-modified-id=\"Stock-Prices-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Stock Prices</a></span></li><li><span><a href=\"#Calculate-Correlation\" data-toc-modified-id=\"Calculate-Correlation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Calculate Correlation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:30:59.804887Z",
     "start_time": "2021-02-13T02:30:59.297351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect S&P 500 Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:31:00.240968Z",
     "start_time": "2021-02-13T02:30:59.805895Z"
    }
   },
   "outputs": [],
   "source": [
    "table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "sandp_df = table[0]\n",
    "\n",
    "#sandp_df.to_csv('data/S&P500-Info.csv')\n",
    "#sandp_df.to_csv(\"data/S&P500-Symbols.csv\", columns=['Symbol'])\n",
    "\n",
    "#https://medium.com/wealthy-bytes/5-lines-of-python-to-automate-getting-the-s-p-500-95a632e5e567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:31:00.254870Z",
     "start_time": "2021-02-13T02:31:00.242900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>SEC filings</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date first added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "      <td>reports</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>St. Paul, Minnesota</td>\n",
       "      <td>1976-08-09</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>reports</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1964-03-31</td>\n",
       "      <td>1800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie Inc.</td>\n",
       "      <td>reports</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Pharmaceuticals</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABMD</td>\n",
       "      <td>Abiomed</td>\n",
       "      <td>reports</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>Danvers, Massachusetts</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>815094</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>reports</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>1467373</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol             Security SEC filings             GICS Sector  \\\n",
       "0    MMM           3M Company     reports             Industrials   \n",
       "1    ABT  Abbott Laboratories     reports             Health Care   \n",
       "2   ABBV          AbbVie Inc.     reports             Health Care   \n",
       "3   ABMD              Abiomed     reports             Health Care   \n",
       "4    ACN            Accenture     reports  Information Technology   \n",
       "\n",
       "                GICS Sub-Industry    Headquarters Location Date first added  \\\n",
       "0        Industrial Conglomerates      St. Paul, Minnesota       1976-08-09   \n",
       "1           Health Care Equipment  North Chicago, Illinois       1964-03-31   \n",
       "2                 Pharmaceuticals  North Chicago, Illinois       2012-12-31   \n",
       "3           Health Care Equipment   Danvers, Massachusetts       2018-05-31   \n",
       "4  IT Consulting & Other Services          Dublin, Ireland       2011-07-06   \n",
       "\n",
       "       CIK      Founded  \n",
       "0    66740         1902  \n",
       "1     1800         1888  \n",
       "2  1551152  2013 (1888)  \n",
       "3   815094         1981  \n",
       "4  1467373         1989  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandp_df.head(5)\n",
    "# so the symbol is the same as the corresponding stock ticker. \n",
    "# It will be used for parsing news results that reference the company that made the headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example code\n",
    "\n",
    "Taken from https://towardsdatascience.com/sentiment-analysis-of-stocks-from-financial-news-using-python-82ebdcefb638"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "''' function to read news table from finviz '''\n",
    "def finviz_news_table(ticker):\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    try:\n",
    "        url = finviz_url + ticker\n",
    "        req = Request(url=url, headers={'user-agent': 'my-app/0.0.1'})\n",
    "        response = urlopen(req)\n",
    "        html = BeautifulSoup(response)\n",
    "        news_table = html.find(id='news-table')\n",
    "    except:\n",
    "        news_table = None\n",
    "        \n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    return [ticker, news_table, end_time - start_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read news table from finviz (use for process pool executor)\n",
    "def finviz_news_table_process(ticker):\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    \n",
    "    try:\n",
    "        url = finviz_url + ticker\n",
    "        req = Request(url=url, headers={'user-agent': 'my-app/0.0.1'})\n",
    "        response = urlopen(req)\n",
    "        html = BeautifulSoup(response)\n",
    "        news_table = str(html.find(id='news-table'))\n",
    "    except:\n",
    "        news_table = None\n",
    "        \n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    # Return [ticker, str_news_table, run_time, pid]\n",
    "    return [ticker, news_table, end_time - start_time, pid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "''' Thread Pool Executor: read html from finviz for each ticker and save the news_table of each as a dataframe '''\n",
    "''' runs slower than process pool executor '''\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "\n",
    "start_test1 = time.perf_counter()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "    ticker_list = sandp_df['Symbol']\n",
    "    # initiate executor\n",
    "    executor = ThreadPoolExecutor()\n",
    "    # apply executor to map finviz_news_table on ticker list \n",
    "    p = executor.map(finviz_news_table, ticker_list)\n",
    "    # save news tables as a dataframe (includes run time for each request)\n",
    "    news_table_df_test1 = pd.DataFrame([[ticker, run_time, news_table] for ticker, news_table, run_time in p], columns=['ticker', 'html_news_table', 'run_time'])\n",
    "    print(news_table_df_test1.head(10))\n",
    "\n",
    "end_test1 = time.perf_counter()\n",
    "\n",
    "print('Thread Pool Executor finished in: ', end_test1 - start_test1, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker    pid   run_time                                     str_news_table\n",
      "0    MMM   2464   5.640783  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "1    ABT  16096   5.560090  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "2   ABBV    548   5.181580  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "3   ABMD  12368   7.059482  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "4    ACN  11536   5.758770  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "5   ATVI  16396   9.440227  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "6   ADBE  12452  11.734430  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "7    AMD   1632  11.684490  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "8    AAP    548   7.100792  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "9    AES  16096   5.570064  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "Process Pool Executor finished in:  280.5679949  seconds\n"
     ]
    }
   ],
   "source": [
    "# Process Pool Executor: read html from finviz for each ticker and save the news_table of each as a dataframe\n",
    "\n",
    "from loky import get_reusable_executor\n",
    "import time\n",
    "\n",
    "\n",
    "start_test1 = time.perf_counter()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "    ticker_list = sandp_df['Symbol']\n",
    "    # initiate executor\n",
    "    executor = get_reusable_executor(max_workers=10, timeout=2)\n",
    "    # apply executor to map finviz_news_table on ticker list \n",
    "    process_1 = executor.map(finviz_news_table_process, ticker_list)\n",
    "    # save news tables as a dataframe (includes run time for each request)\n",
    "    news_table_df = pd.DataFrame([[ticker, pid, run_time, str_news_table] for ticker, str_news_table, run_time, pid in process_1], columns=['ticker', 'pid', 'run_time', 'str_news_table'])\n",
    "    print(news_table_df.head(10))\n",
    "    \n",
    "\n",
    "end_test1 = time.perf_counter()\n",
    "\n",
    "print('Process Pool Executor finished in: ', end_test1 - start_test1, ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan for next section\n",
    "\n",
    "Single layer multiprocessing with thread (parallel inside loop outside)  # in progress\n",
    "\n",
    "Single layer multiprocessing with process\n",
    "\n",
    "Single layer multiprocessing with thread (parallel outside loop inside)\n",
    "\n",
    "Single layer multiprocessing with process\n",
    "\n",
    "Double layer multiprocessing with thread (both)\n",
    "\n",
    "Double layer multiprocessing with process\n",
    "\n",
    "Double layer multiprocessing (thread inside, process outside)\n",
    "\n",
    "Double layer multiprocessing (thread outside, process inside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single layer with thread (parallel inside loop outside)\n",
    "Note: In synchronous execution, soupifying the response takes up an overwhelming amount of the total processing time(93%). 2nd highest is waiting for replies at 6%.\n",
    "\n",
    "Based on actual run time, thread appears to significantly save the soupifying time (by 90+%). Theoretically it should save the waiting time as well.\n",
    "\n",
    "Test run time = 191.48199870000008 seconds\n",
    "Estimated full run time = 322.0 minutes 19.68 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# define function to generate article content (in a list) given a row in news_table_df\\n# only works with articles from yahoo.finance right now\\ndef article_content(html_article):\\n    # get the link to the full article\\n    link = html_article.find('a').get('href')\\n    content = 'empty string'\\n    # check if link leads to yahoo.finance\\n    if urlparse(link).netloc == 'finance.yahoo.com':\\n        try:\\n            # request from yahoo.finance\\n            req_art = Request(url=link, headers={'user-agent':'my-app/0.0.1'})\\n            response_art = urlopen(req_art)\\n            html_art = BeautifulSoup(response_art)\\n            # get the article content\\n            content = str(html_art.find(class_='caas-body').get_text())\\n        except:\\n            print('Error following article link: ', link)\\n    else:\\n        return [content, urlparse(link).netloc]\\n    return [content]\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define function to generate [date, time, headline, news_source, content, article_site, run_time] \n",
    "# input is a single article\n",
    "def article_details(str_article):\n",
    "    \n",
    "    art_det_start = time.perf_counter()\n",
    "    \n",
    "    \n",
    "    # convert str to html\n",
    "    html_article = BeautifulSoup(str_article, 'html.parser')\n",
    "    \n",
    "    # Produce headlines\n",
    "    headline = html_article.a.get_text() \n",
    "    \n",
    "    # Produce news source company\n",
    "    news = html_article.span.get_text()\n",
    "    \n",
    "    # Produce Date and Time\n",
    "    # split text in the td tag into a list \n",
    "    date_scrape = html_article.td.text.split()\n",
    "    # ensure most recent date is used\n",
    "    global date\n",
    "    # if the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "    if len(date_scrape) == 1:\n",
    "        time_ = date_scrape[0]\n",
    "    # else load 'date' as the 1st element and 'time' as the second    \n",
    "    else:\n",
    "        date = date_scrape[0]\n",
    "        time_ = date_scrape[1]\n",
    "        \n",
    "    req_start = 0\n",
    "    req_end = 0\n",
    "    wait_end = 0\n",
    "    soup_end = 0\n",
    "    cont_end = 0\n",
    "    \n",
    "    # Produce news content\n",
    "    # get link to the full article\n",
    "    link = html_article.find('a').get('href')\n",
    "    content = 'empty string'\n",
    "    url_root = urlparse(link).netloc\n",
    "    # check if link leads to yahoo.finance\n",
    "    if url_root == 'finance.yahoo.com':\n",
    "        try:\n",
    "            # request from yahoo.finance\n",
    "            req_start = time.perf_counter()\n",
    "            req_art = Request(url=link, headers={'user-agent':'my-app/0.0.1'})\n",
    "            req_end = time.perf_counter()\n",
    "            \n",
    "            response_art = urlopen(req_art)\n",
    "            wait_end = time.perf_counter()\n",
    "            \n",
    "            html_art = BeautifulSoup(response_art)\n",
    "            soup_end = time.perf_counter()\n",
    "            # get the article content\n",
    "            content = str(html_art.find(class_='caas-body').get_text())\n",
    "            cont_end = time.perf_counter()\n",
    "        except:\n",
    "            print('Error following article link: ', link)\n",
    "    \n",
    "    \n",
    "    art_det_end = time.perf_counter()\n",
    "    \n",
    "    \n",
    "    # Return [date, time, headline, news_source, content, article_site, run_time, req_time, wait_time, soup_time, cont_time] \n",
    "    return [date, time_, headline, news, content, url_root, art_det_end - art_det_start, req_end - req_start, wait_end - req_end, soup_end - wait_end, cont_end - soup_end]\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# define function to generate article content (in a list) given a row in news_table_df\n",
    "# only works with articles from yahoo.finance right now\n",
    "def article_content(html_article):\n",
    "    # get the link to the full article\n",
    "    link = html_article.find('a').get('href')\n",
    "    content = 'empty string'\n",
    "    # check if link leads to yahoo.finance\n",
    "    if urlparse(link).netloc == 'finance.yahoo.com':\n",
    "        try:\n",
    "            # request from yahoo.finance\n",
    "            req_art = Request(url=link, headers={'user-agent':'my-app/0.0.1'})\n",
    "            response_art = urlopen(req_art)\n",
    "            html_art = BeautifulSoup(response_art)\n",
    "            # get the article content\n",
    "            content = str(html_art.find(class_='caas-body').get_text())\n",
    "        except:\n",
    "            print('Error following article link: ', link)\n",
    "    else:\n",
    "        return [content, urlparse(link).netloc]\n",
    "    return [content]\n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record time to parse through each ticker and generate necessary details\n",
    "#time_per_ticker_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to prepare dataframe with [ticker, date, time, headline, news, content, article_site, run_time, req_time, wait_time, soup_time, cont_time]\n",
    "# Thread Pool Processor\n",
    "\n",
    "# input is a row [ticker, pid, run_time, str_news_table] from news_table_df\n",
    "# return completed dataframe for 1 ticker (to be appended)\n",
    "# also records time to process 1 ticker and saves in list 'time_per_ticker'\n",
    "\n",
    "def ticker_to_dataframe_thr(row):\n",
    "    \n",
    "    ticker_time_start = time.perf_counter()\n",
    "    \n",
    "    \n",
    "    # convert str_news_table to html format\n",
    "    html_news_table = BeautifulSoup(row[3], 'html.parser')\n",
    "    # split into list of articles in html format\n",
    "    article_list = html_news_table.findAll('tr')\n",
    "    # convert all html to str\n",
    "    article_list = [str(x) for x in article_list]\n",
    "    \n",
    "    # executor\n",
    "    if __name__ == '__main__':\n",
    "        executor = ThreadPoolExecutor()\n",
    "        thread_2 = executor.map(article_details, article_list)\n",
    "        ticker_df = [[date, time_, news_source, headline, content, site, run_time, req_time, wait_time, soup_time, cont_time] for date, time_, headline, news_source, content, site, run_time, req_time, wait_time, soup_time, cont_time in thread_2]\n",
    "        ticker_df = pd.DataFrame(ticker_df, columns=['date', 'time', 'news', 'headline', 'content', 'article_site', 'run_time', 'req_time', 'wait_time', 'soup_time', 'cont_time'])\n",
    "        \n",
    "        \n",
    "    ticker = row[0]\n",
    "    ticker_col = pd.Series([ticker] * len(ticker_df))\n",
    "    \n",
    "    ticker_df.insert(0, 'ticker', ticker_col)\n",
    "        \n",
    "    \n",
    "    ticker_time_end = time.perf_counter()\n",
    "    time_per_ticker_1.append(ticker_time_end - ticker_time_start)\n",
    "\n",
    "    return ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing input\n",
      "  ticker    pid  run_time                                     str_news_table\n",
      "0    MMM   2464  5.640783  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "1    ABT  16096  5.560090  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "2   ABBV    548  5.181580  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "3   ABMD  12368  7.059482  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "4    ACN  11536  5.758770  <table border=\"0\" cellpadding=\"1\" cellspacing=...\n",
      "Ticker complete: MMM\n",
      "Ticker complete:  ABT\n",
      "Ticker complete:  ABBV\n",
      "Ticker complete:  ABMD\n",
      "Ticker complete:  ACN\n",
      "  ticker       date     time             news  \\\n",
      "0    MMM  Feb-14-21  09:27AM      Motley Fool   \n",
      "1    MMM  Jan-26-21  05:12PM    GuruFocus.com   \n",
      "2    MMM  Feb-12-21  10:10AM      Motley Fool   \n",
      "3    MMM  Feb-12-21  07:45AM      Motley Fool   \n",
      "4    MMM  Feb-10-21  06:00AM      Barrons.com   \n",
      "5    MMM  Oct-29-20  02:35PM      PR Newswire   \n",
      "6    MMM  Jan-26-21  11:07AM      PR Newswire   \n",
      "7    MMM  Oct-29-20  11:07AM        CNW Group   \n",
      "8    MMM  Feb-08-21  10:30AM      Motley Fool   \n",
      "9    MMM  Oct-29-20  09:12AM   Insider Monkey   \n",
      "\n",
      "                                            headline  \\\n",
      "0    These 3 Dividend Stocks Are Too Cheap to Ignore   \n",
      "1    First Eagle Investment's Top 4th-Quarter Trades   \n",
      "2                  Why 3M Is a Retiree's Dream Stock   \n",
      "3                3 Top Value Stocks to Buy Right Now   \n",
      "4  How a $1.9B Bond Fund Finds Opportunity in Mar...   \n",
      "5              3M Announces Upcoming Investor Events   \n",
      "6  New 3M Polisher ST reduces the number of bioph...   \n",
      "7  New 3M Polisher ST reduces the number of bioph...   \n",
      "8  3 Stocks to Buy With Dividends Yielding More T...   \n",
      "9               30 Dividend Kings of 2021 (Part III)   \n",
      "\n",
      "                                             content       article_site  \\\n",
      "0                                       empty string       www.fool.com   \n",
      "1  - By Graham GriffinFirst Eagle Investment (Tra...  finance.yahoo.com   \n",
      "2                                       empty string       www.fool.com   \n",
      "3                                       empty string       www.fool.com   \n",
      "4                                       empty string    www.barrons.com   \n",
      "5  ST. PAUL, Minn., Feb. 9, 2021 /PRNewswire/ -- ...  finance.yahoo.com   \n",
      "6  ST. PAUL, Minn., Feb. 9, 2021 /PRNewswire/ -- ...  finance.yahoo.com   \n",
      "7  ST. PAUL, Minn., Feb. 9, 2021 /CNW/ -- 3M Heal...  finance.yahoo.com   \n",
      "8                                       empty string       www.fool.com   \n",
      "9  This is the last part of Insider Monkey's thre...  finance.yahoo.com   \n",
      "\n",
      "    run_time  req_time  wait_time  soup_time  cont_time  \n",
      "0   0.001385  0.000000   0.000000   0.000000   0.000000  \n",
      "1   6.568764  0.000083   0.212501   6.336542   0.018062  \n",
      "2   0.001064  0.000000   0.000000   0.000000   0.000000  \n",
      "3   0.000622  0.000000   0.000000   0.000000   0.000000  \n",
      "4   0.001012  0.000000   0.000000   0.000000   0.000000  \n",
      "5  30.454953  0.000042   0.182514  30.263779   0.007639  \n",
      "6   6.216005  0.000035   0.180754   6.016597   0.017644  \n",
      "7  27.182128  0.000042   0.134128  27.038292   0.008593  \n",
      "8   0.001115  0.000000   0.000000   0.000000   0.000000  \n",
      "9  29.219103  0.000028   0.236430  28.971666   0.010348  \n",
      "Time taken to compile 5 tickers is:  191.48199870000008  seconds\n",
      "Estimated time to compile all tickers is:  322.0  minutes 19.681868700008636  seconds\n"
     ]
    }
   ],
   "source": [
    "# test run \n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# define test df\n",
    "test_news_table_df = news_table_df.copy()\n",
    "test_news_table_df = test_news_table_df.iloc[0:5]\n",
    "print('Testing input')\n",
    "print(test_news_table_df)\n",
    "\n",
    "time_per_ticker_1 = []\n",
    "\n",
    "compile_start = time.perf_counter()\n",
    "\n",
    "test_art_det_df = ticker_to_dataframe_thr(test_news_table_df.iloc[0])\n",
    "print('Ticker complete: MMM')\n",
    "\n",
    "\n",
    "for i in range(1, len(test_news_table_df)):\n",
    "    test_ticker_df = ticker_to_dataframe_thr(test_news_table_df.iloc[i])\n",
    "    test_art_det_df = test_art_det_df.append(test_ticker_df)\n",
    "    print('Ticker complete: ', test_ticker_df.iloc[0,0])\n",
    "\n",
    "compile_end = time.perf_counter()\n",
    "    \n",
    "print(test_art_det_df.head(10))\n",
    "print('Time taken to compile 5 tickers is: ', compile_end - compile_start, ' seconds')\n",
    "minutes = ((compile_end - compile_start) * len(news_table_df)/5 ) // 60 \n",
    "seconds = ((compile_end - compile_start) * len(news_table_df)/5 ) % 60\n",
    "print('Estimated time to compile all tickers is: ', minutes, 'minutes', seconds, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total art det time is:  3427.004503999985\n",
      "Total time series\n",
      "[0.011493199994220049, 207.07549440000457, 3215.8481493000018, 3.728625999999167]\n",
      "Fractional time series\n",
      "[3.353716045836892e-06, 0.0604246344462947, 0.9383845704160814, 0.0010880131600781763]\n"
     ]
    }
   ],
   "source": [
    "# synchronous run times\n",
    "\n",
    "art_det_total_time = sum(test_art_det_df['run_time'])\n",
    "gen_req_total_time = sum(test_art_det_df['req_time'])\n",
    "wait_total_time = sum(test_art_det_df['wait_time'])\n",
    "soup_total_time = sum(test_art_det_df['soup_time'])\n",
    "cont_total_time = sum(test_art_det_df['cont_time'])\n",
    "\n",
    "time_series = [gen_req_total_time, wait_total_time, soup_total_time, cont_total_time]\n",
    "frac_time_series = [x / art_det_total_time for x in time_series]\n",
    "\n",
    "\n",
    "print('Total art det time is: ', art_det_total_time)\n",
    "print('Total time series')\n",
    "print(time_series)\n",
    "print('Fractional time series')\n",
    "print(frac_time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3028.524503999985"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimated time saved from soupifying\n",
    "art_det_total_time - 191.48 - 207"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single layer with process (parallel inside loop outside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to prepare data with ticker, date, time, headline, news, content\n",
    "# Process Pool Processor\n",
    "\n",
    "# input is a row [ticker, pid, run_time, str_news_table]\n",
    "# return completed dataframe for 1 ticker (to be appended)\n",
    "# also records time to process 1 ticker and saves in list 'time_per_ticker'\n",
    "\n",
    "def ticker_to_dataframe_pro(row):\n",
    "    \n",
    "    ticker_time_start = time.perf_counter()\n",
    "    \n",
    "    \n",
    "    ticker_pid = os.get_pid()\n",
    "    \n",
    "    # convert str_news_table to html format\n",
    "    html_news_table = BeautifulSoup(row[3], 'html.parser')\n",
    "    \n",
    "    # section of code to be looped\n",
    "    if __name__ == '__main__':\n",
    "        executor = \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    for file_name, news_str in html_dict.items():\n",
    "        # soupify str\n",
    "        news_html = BeautifulSoup(news_str, 'html.parser')\n",
    "        # iterate through all tr tags in 'news_table'\n",
    "        for x in news_html.findAll('tr'):\n",
    "            # article content and rejected site (if applicable) (use to expand available sites in the future)\n",
    "            art_content = article_content(x)\n",
    "            ticker = file_name.split('_')[0]\n",
    "            # combine article summary and article content into 1 row of info about the article\n",
    "            art_row = [ticker] + article_summary(x) + [art_content[0]]\n",
    "            # append relevant info to the 'parsed_news' list\n",
    "            parsed_news.append(art_row)\n",
    "            if len(art_content) > 1:\n",
    "                rejected_sites.append(art_content[1])\n",
    "        print('Done parsing through: ', ticker)\n",
    "        \n",
    "    \n",
    "    ticker_time_end = time.perf_counter()\n",
    "    time_per_ticker.append(ticker_time_end - ticker_time_start)\n",
    "\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Pool Executor\n",
    "\n",
    "from loky import get_reusable_executor\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "start_test1 = time.perf_counter()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_news_tables = news_tables.copy()\n",
    "    test_news_tables = dict(zip(['MMM', 'ABT', 'ABBV'], [str(test_news_tables[x]) for x in ['MMM', 'ABT', 'ABBV']]))\n",
    "    executor = get_reusable_executor(max_workers=4, timeout=2)\n",
    "    p = executor.submit(prepare_data, test_news_tables)\n",
    "    print('Processor ID: ', p.result())\n",
    "        \n",
    "        \n",
    "columns = ['ticker', 'date', 'time', 'headline', 'news', 'content']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_news_updated'\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "ori_len = len(parsed_news_updated)\n",
    "# remove articles whose contents are not available\n",
    "parsed_news_updated_cont = parsed_news_updated.copy()\n",
    "parsed_news_updated_cont = parsed_news_updated_cont.loc[parsed_news_updated_cont['content'] != 'empty string']\n",
    "\n",
    "parsed_news_updated = parsed_news_updated.drop('content', axis=1)\n",
    "\n",
    "print('Percentage decrease in articles is: ', (ori_len - len(parsed_news_updated_cont))/ori_len)\n",
    "\n",
    "\n",
    "print(parsed_news_updated_cont.head(10))\n",
    "print(rejected_sites[:20])\n",
    "\n",
    "end_test1 = time.perf_counter()\n",
    "\n",
    "print('Process Pool Executor finished in: ', end_test1 - start_test1, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread Pool Executor\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "start_test1 = time.perf_counter()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_news_tables = news_tables.copy()\n",
    "    test_news_tables = dict(zip(['MMM', 'ABT', 'ABBV'], [str(test_news_tables[x]) for x in ['MMM', 'ABT', 'ABBV']]))\n",
    "    executor = ThreadPoolExecutor()\n",
    "    p = executor.submit(prepare_data, test_news_tables)\n",
    "    print(p.result())\n",
    "        \n",
    "        \n",
    "columns = ['ticker', 'date', 'time', 'headline', 'news', 'content']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_news_updated'\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "ori_len = len(parsed_news_updated)\n",
    "# remove articles whose contents are not available\n",
    "parsed_news_updated_cont = parsed_news_updated.copy()\n",
    "parsed_news_updated_cont = parsed_news_updated_cont.loc[parsed_news_updated_cont['content'] != 'empty string']\n",
    "\n",
    "parsed_news_updated = parsed_news_updated.drop('content', axis=1)\n",
    "\n",
    "print('Percentage decrease in articles is: ', (ori_len - len(parsed_news_updated_cont))/ori_len)\n",
    "\n",
    "\n",
    "print(parsed_news_updated_cont.head(10))\n",
    "print(rejected_sites[:20])\n",
    "\n",
    "end_test1 = time.perf_counter()\n",
    "\n",
    "print('Thread Pool Executor finished in: ', end_test1 - start_test1, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "start_test2 = time.perf_counter()\n",
    "\n",
    "\n",
    "test_news_tables = news_tables.copy()\n",
    "test_news_tables = dict(zip(['MMM', 'ABT', 'ABBV'], [str(test_news_tables[x]) for x in ['MMM', 'ABT', 'ABBV']]))\n",
    "print(prepare_data(test_news_tables))\n",
    "\n",
    "# Set column names\n",
    "columns = ['ticker', 'date', 'time', 'headline', 'news', 'content']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "ori_len = len(parsed_news_updated)\n",
    "# remove articles whose contents are not available\n",
    "parsed_news_updated_content = parsed_news_updated.copy()\n",
    "parsed_news_updated_content = parsed_news_updated_content.loc[parsed_news_updated['content'] != 'empty string']\n",
    "parsed_news_updated = parsed_news_updated.drop('content', axis=1)\n",
    "print('Percentage decrease in articles is: ', (ori_len - len(parsed_news_updated_content))/ori_len)\n",
    "\n",
    "\n",
    "print(parsed_news_updated_content.head(10))\n",
    "print(rejected_sites[:20])\n",
    "\n",
    "\n",
    "end_test2 = time.perf_counter()\n",
    "\n",
    "print('Synchronous test finished in: ', end_test2 - start_test2, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:34:38.201871Z",
     "start_time": "2021-02-13T02:34:34.857909Z"
    }
   },
   "outputs": [],
   "source": [
    "parsed_news = []\n",
    "rejected_sites = []\n",
    "\n",
    "# Iterate through the news\n",
    "for file_name, news_table in news_tables.items():\n",
    "    # Iterate through all tr tags in 'news_table'\n",
    "    for x in news_table.findAll('tr'):\n",
    "        art_content = article_content(x)\n",
    "        # combine article summary and article content into 1 row of info about the article\n",
    "        article_row = article_summary(x) + [art_content[0]]\n",
    "        # Append relevant info as a list to the 'parsed_news' list\n",
    "        parsed_news.append(article_row)\n",
    "        rejected_sites.append(art_content[1])\n",
    "\n",
    "# Set column names\n",
    "columns = ['ticker', 'date', 'time', 'headline', 'news', 'content']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "ori_len = len(parsed_news_updated)\n",
    "# remove articles whose contents are not available\n",
    "parsed_news_updated_content = parsed_news_updated.copy()\n",
    "parsed_news_updated_content = parsed_news_updated_content.loc[parsed_news_updated['content'] != 'empty string']\n",
    "parsed_news_updated = parsed_news_updated.drop('content', axis=1)\n",
    "print('Percentage decrease in articles is: ', (ori_len - len(parsed_news_updated_content))/ori_len)\n",
    "parsed_news_updated_content.head(10)\n",
    "\n",
    "#parsed_news_updated does not have the content column and has full number of articles (100 per ticker)\n",
    "#parsed_news_updated_content has content column and is shortened to remove all rows with no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of headlines produced by each news source and remove news sources with <median headlines\n",
    "parsed_news_updated = pd.DataFrame(parsed_news, columns=columns)\n",
    "# generate series of counts\n",
    "news_count = parsed_news_updated['news'].value_counts()\n",
    "# set minimum count = median\n",
    "cutoff_point = news_count.median()\n",
    "# append count of news to dataframe\n",
    "parsed_news_updated = parsed_news_updated.merge(news_count, left_on='news', right_index=True)\n",
    "parsed_news_updated = parsed_news_updated.drop('news_x', axis=1).rename({'news_y' : 'count'}, axis=1)\n",
    "# remove news which have count < cutoff_point\n",
    "parsed_news_updated = parsed_news_updated.loc[parsed_news_updated['count'] > cutoff_point]\n",
    "parsed_news_updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:34:48.450433Z",
     "start_time": "2021-02-13T02:34:41.894994Z"
    }
   },
   "outputs": [],
   "source": [
    "#need to tokenize each words within the headlines to improve the sentiment score.\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def regex(x):\n",
    "    special_chars_p = \"[.®'&$’\\\"\\-()#@!?/:]\"\n",
    "    s1 = re.sub(special_chars_p, '', x)  \n",
    "    return(s1)\n",
    "\n",
    "parsed_news_updated['headline'] = parsed_news_updated['headline'].apply(regex)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.lower().split()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "parsed_news_updated_stem = parsed_news_updated.copy()\n",
    "parsed_news_updated_stem['headline'] = parsed_news_updated_stem['headline'].apply(stem_sentences)\n",
    "\n",
    "stop=stopwords.words('english')\n",
    "\n",
    "parsed_news_updated['headline'].apply(lambda x: [item for item in x if item not in stop])\n",
    "parsed_news_updated_stem['headline'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "parsed_news_updated['headline'] = parsed_news_updated['headline'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) \n",
    "parsed_news_updated_stem['headline'] = parsed_news_updated_stem['headline'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) \n",
    "parsed_news_updated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:34:56.695664Z",
     "start_time": "2021-02-13T02:34:50.966410Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NLTK VADER for sentiment analysis (unstem)\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Instantiate the sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Iterate through the headlines and get the polarity scores using vader\n",
    "scores = parsed_news_updated['headline'].apply(vader.polarity_scores).tolist()\n",
    "\n",
    "# Convert the 'scores' list of dicts into a DataFrame\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# Join the DataFrames of the news and the list of dicts\n",
    "parsed_and_scored_news = parsed_news_updated.join(scores_df, rsuffix='_right')\n",
    "\n",
    "# Convert the date column from string to datetime\n",
    "parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.date\n",
    "\n",
    "parsed_and_scored_news.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK VADER for sentiment analysis (stem)\n",
    "\n",
    "# Instantiate the sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Iterate through the headlines and get the polarity scores using vader\n",
    "scores_stem = parsed_news_updated_stem['headline'].apply(vader.polarity_scores).tolist()\n",
    "\n",
    "# Convert the 'scores' list of dicts into a DataFrame\n",
    "scores_stem_df = pd.DataFrame(scores_stem)\n",
    "\n",
    "# Join the DataFrames of the news and the list of dicts\n",
    "parsed_and_scored_news_stem = parsed_news_updated_stem.join(scores_stem_df, rsuffix='_right')\n",
    "\n",
    "# Convert the date column from string to datetime\n",
    "parsed_and_scored_news_stem['date'] = pd.to_datetime(parsed_and_scored_news_stem.date).dt.date\n",
    "\n",
    "parsed_and_scored_news_stem.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:35:12.704918Z",
     "start_time": "2021-02-13T02:35:12.633876Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get S&P 500 prices\n",
    "# source: https://www.spglobal.com/spdji/en/indices/equity/sp-500/#overview\n",
    "\n",
    "df_sp = pd.read_csv('data/S&P500_5years.csv', usecols=[0,1]) # Use only first 2 columns\n",
    "df_sp.columns = ['date', 'price']\n",
    "df_sp['date'] = pd.to_datetime(df_sp['date'])\n",
    "df_sp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:36:17.050503Z",
     "start_time": "2021-02-13T02:35:14.178417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get S&P 500 individual stock prices\n",
    "\n",
    "# Create a function to get stock price given a ticker \n",
    "def get_stock_price(ticker, start, end):\n",
    "    '''Get prices of a stock in a given period.\n",
    "    \n",
    "    Args:\n",
    "        ticker (str): ticker of a company \n",
    "        start (str): date in format of 'YYYY-MM-DD'\n",
    "        end (str): date in format of 'YYYY-MM-DD'\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing open, high, low, close, volume, dividends, stock splits\n",
    "    '''\n",
    "    import yfinance as yf\n",
    "    \n",
    "    ticker = yf.Ticker(ticker)\n",
    "    data = ticker.history(start=start, end=end)\n",
    "    data.reset_index(level=0, inplace=True)\n",
    "    return data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate merged dataframe (merged by date)\n",
    "# columns: ['ticker', 'date', 'time', 'headline', 'news', 'neg', 'neu', 'pos', 'compound', 'open', 'close', 'change']\n",
    "# scored dataframe should be the input (not sure if works for scored sentiment other than Vader)\n",
    "\n",
    "\n",
    "def generate_final_df(scored_df):\n",
    "    # Get a list of 505 stocks from S&P 500\n",
    "    sp500 = sandp_df['Symbol'].unique()\n",
    "    start = scored_df['date'].min()\n",
    "    end = scored_df['date'].max()\n",
    "    \n",
    "    # Iterate through each stock to get price\n",
    "    df_stock = pd.DataFrame()\n",
    "    for ticker in sp500:\n",
    "        data = get_stock_price(ticker, start, end)\n",
    "        data['ticker'] = ticker\n",
    "        df_stock = pd.concat([df_stock, data], axis=0)\n",
    "        \n",
    "    # Change all columns names to lowercase  \n",
    "    df_stock.columns = df_stock.columns.str.lower()\n",
    "    \n",
    "    # Convert timestamp to date\n",
    "    df_stock['date'] = df_stock['date'].apply(datetime.date)\n",
    "    \n",
    "    # Reset index\n",
    "    df_stock.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Merge stock price info and sentiment scores\n",
    "    df_merged = scored_df.merge(df_stock.loc[:, ['date', 'ticker', 'open', 'close']], on=['date', 'ticker'])\n",
    "    # Add column: price change\n",
    "    df_merged['change'] = df_merged['close'] - df_merged['open']\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final df for unstem and stem\n",
    "\n",
    "df_final_unstem = generate_final_df(parsed_and_scored_news)\n",
    "df_final_stem = generate_final_df(parsed_and_scored_news_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:37:43.967232Z",
     "start_time": "2021-02-13T02:37:43.917201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate pearson correlation coef between sentiment score and price for each news media\n",
    "scores_close_unstem = df_final_unstem.groupby('news')[['compound', 'close']].corr().unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_close_stem = df_final_stem.groupby('news')[['compound', 'close']].corr().unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_change_unstem = df_final_unstem.groupby('news')[['compound', 'change']].corr().unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_change_stem = df_final_stem.groupby('news')[['compound', 'change']].corr().unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/28988627/pandas-correlation-groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate spearman correlation coef between sentiment score and price for each news media\n",
    "scores_close_unstem = df_final_unstem.groupby('news')[['compound', 'close']].corr(method='spearman').unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_close_stem = df_final_stem.groupby('news')[['compound', 'close']].corr(method='spearman').unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_change_unstem = df_final_unstem.groupby('news')[['compound', 'change']].corr(method='spearman').unstack().iloc[:, 1].sort_values(ascending=False)\n",
    "scores_change_stem = df_final_stem.groupby('news')[['compound', 'change']].corr(method='spearman').unstack().iloc[:, 1].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson's Correlation Coefficient as Dataframe\n",
    "pearson_corr = pd.DataFrame({'variable' : ['close', 'change'], 'unstem' : [df_final_unstem[['compound', 'close']].corr().iloc[0,1], df_final_unstem[['compound', 'change']].corr().iloc[0,1]], 'stem' : [df_final_stem[['compound', 'close']].corr().iloc[0,1], df_final_stem[['compound', 'change']].corr().iloc[0,1]]})\n",
    "pearson_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman's rank correlation\n",
    "spearman_corr = pd.DataFrame({'variable' : ['close', 'change'], 'unstem' : [df_final_unstem[['compound', 'close']].corr(method='spearman').iloc[0,1], df_final_unstem[['compound', 'change']].corr(method='spearman').iloc[0,1]], 'stem' : [df_final_stem[['compound', 'close']].corr(method='spearman').iloc[0,1], df_final_stem[['compound', 'change']].corr(method='spearman').iloc[0,1]]})\n",
    "spearman_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vader Sentiment Analysis\n",
    "\n",
    "Performed Pearson's Correlation Coefficient comparisons between compound sentiment and (1) closing price (2) change in price (closing price - opening price). Computed the Spearman rank correlation coefficient as well.\n",
    "\n",
    "Compared effect of stemming and not stemming words on Pearson's and Spearman's correlation coefficient.\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Pearson:\n",
    "Correlation between both (1) and (2) is negligible (<1%) without stemming. Stemming appears to improve correlation, but correlation is still very small (<2%)\n",
    "\n",
    "Spearman:\n",
    "Correlation for both (1) and (2) is still small, but better than Pearson. Stemming has inconsistent results, slightly lowering (2) but increasing (1)\n",
    "\n",
    "Overall, Vader sentiment analysis produces very weak correlation with both (1) and (2). Try with other models.\n",
    "\n",
    "Removing headlines from news sources with less articles:\n",
    "Decreased correlation. Most of the correlations are negative but close to 0. This suggests that news sources with lower article counts tend to predict the price movements more accurately than the news sources that post more often.\n",
    "\n",
    "\n",
    "Note: \n",
    "\n",
    "1. Might be helpful to determine the most relevant news sources by taking highly correlated news sources with instances of more than 20. Limiting the data might increase correlation. Use test set to evaluate if using this method.\n",
    "\n",
    "2. Doing linear regression on neg, neu and pos score might produce interesting results.\n",
    "\n",
    "3. Vader sentiment scores might be the problem. Sentiment scores of some sample headlines was observed and Vader had many false negatives. Try using article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T02:38:23.792228Z",
     "start_time": "2021-02-13T02:38:23.778225Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final_unstem.loc[df_final_unstem['news']==' The Telegraph', ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "1. Take note of changes in the composition of S&P 500."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
